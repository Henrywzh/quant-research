{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from experiments.backtest import *\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "cf6f0e699906900b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE   = \"2025-01-15\"\n",
    "# \n",
    "# hsi_stocks = pd.read_excel(\"/Users/henrywzh/Desktop/hk_stock_ejfq.xlsx\")\n",
    "# \n",
    "# def code_int_to_hk(code: int) -> str:\n",
    "#     return f\"{int(code):04d}.HK\"\n",
    "# \n",
    "# UNIVERSE = hsi_stocks[\"代码\"].apply(code_int_to_hk).tolist()\n",
    "# \n",
    "# price_df = yf.download(UNIVERSE, start=START_DATE, end=END_DATE)\n",
    "# hsi = yf.download(\"^HSI\", START_DATE, END_DATE)\n",
    "# \n",
    "# # Extract close matrices\n",
    "# close = price_df[\"Close\"].sort_index()\n",
    "# benchmark_close = hsi[\"Close\"].sort_index()\n",
    "# \n",
    "# # Common calendar (optional but recommended)\n",
    "# common_dates = close.index.intersection(benchmark_close.index)\n",
    "# close = close.loc[common_dates]\n",
    "# benchmark_close = benchmark_close.loc[common_dates]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sp500_stocks = pd.read_excel('/Users/henrywzh/Desktop/us_indices.xlsx', sheet_name='sp500')\n",
    "sp400_stocks = pd.read_excel('/Users/henrywzh/Desktop/us_indices.xlsx', sheet_name='sp400')"
   ],
   "id": "176e5dd221259004",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "price_df = yf.download(list(sp400_stocks['Ticker']), start=START_DATE, end=END_DATE)",
   "id": "f9932e5939ee44f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ticker = '^GSPC'\n",
    "\n",
    "benchmark = yf.download(ticker, start=START_DATE, end=END_DATE)\n",
    "benchmark_close = benchmark['Close']"
   ],
   "id": "4337ea69aa5197a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_ma_signal(price_df, windows=(5, 20, 50, 120)):\n",
    "    close = price_df[\"Close\"].sort_index()\n",
    "\n",
    "    ma_sum = 0.0\n",
    "    for w in windows:\n",
    "        ma = close.rolling(w, min_periods=w).mean()\n",
    "        ma_sum += (close / ma - 1.0)\n",
    "\n",
    "    signal = ma_sum / len(windows)\n",
    "    return signal\n",
    "\n",
    "def build_rsi_signal(price_df: pd.DataFrame, period: int = 14) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    RSI computed per ticker from Close prices.\n",
    "    Returns RSI in [0, 100]. Higher RSI = stronger recent gains.\n",
    "    \"\"\"\n",
    "    close = price_df[\"Close\"].sort_index()\n",
    "\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0.0)\n",
    "    loss = (-delta).clip(lower=0.0)\n",
    "\n",
    "    # Wilder's smoothing: EMA with alpha = 1/period\n",
    "    avg_gain = gain.ewm(alpha=1/period, adjust=False, min_periods=period).mean()\n",
    "    avg_loss = loss.ewm(alpha=1/period, adjust=False, min_periods=period).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100.0 - (100.0 / (1.0 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def build_macd_signal(\n",
    "    price_df: pd.DataFrame,\n",
    "    fast: int = 12,\n",
    "    slow: int = 26,\n",
    "    signal: int = 9,\n",
    "    output: str = \"hist\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    MACD per ticker from Close prices.\n",
    "\n",
    "    output:\n",
    "      - \"line\": MACD line = EMA(fast) - EMA(slow)\n",
    "      - \"signal\": signal line = EMA(MACD line, signal)\n",
    "      - \"hist\": histogram = MACD line - signal line   (common trading signal)\n",
    "    \"\"\"\n",
    "    close = price_df[\"Close\"].sort_index()\n",
    "\n",
    "    ema_fast = close.ewm(span=fast, adjust=False, min_periods=fast).mean()\n",
    "    ema_slow = close.ewm(span=slow, adjust=False, min_periods=slow).mean()\n",
    "\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    macd_sig = macd_line.ewm(span=signal, adjust=False, min_periods=signal).mean()\n",
    "    macd_hist = macd_line - macd_sig\n",
    "\n",
    "    if output == \"line\":\n",
    "        return macd_line\n",
    "    if output == \"signal\":\n",
    "        return macd_sig\n",
    "    if output == \"hist\":\n",
    "        return macd_hist\n",
    "    raise ValueError(\"output must be one of: 'line', 'signal', 'hist'\")\n",
    "\n",
    "\n",
    "def cs_winsorize_zscore(\n",
    "    sig: pd.DataFrame,\n",
    "    lower_q: float = 0.01,\n",
    "    upper_q: float = 0.99,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cross-sectional winsorize + z-score per date.\n",
    "    \"\"\"\n",
    "    def _proc_row(x: pd.Series) -> pd.Series:\n",
    "        x = x.astype(float)\n",
    "        m = x.notna()\n",
    "        if m.sum() < 5:\n",
    "            return x  # too few values\n",
    "        lo = x[m].quantile(lower_q)\n",
    "        hi = x[m].quantile(upper_q)\n",
    "        x2 = x.copy()\n",
    "        x2[m] = x[m].clip(lo, hi)\n",
    "        mu = x2[m].mean()\n",
    "        sd = x2[m].std(ddof=1)\n",
    "        if sd == 0 or not np.isfinite(sd):\n",
    "            return x2\n",
    "        x2[m] = (x2[m] - mu) / sd\n",
    "        return x2\n",
    "\n",
    "    return sig.apply(_proc_row, axis=1)\n"
   ],
   "id": "91f04f2f14c738b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Dict, Optional, Tuple, Any, List\n",
    "\n",
    "\n",
    "def perf_summary(r: pd.Series, freq: float) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"ann_return_geo\": annualised_return_geo(r, freq=freq),\n",
    "        \"ann_vol\": annualised_vol(r, freq=freq),\n",
    "        \"sharpe\": sharpe_ratio(r, freq=freq),\n",
    "        \"max_dd\": max_drawdown(r),\n",
    "        \"n_obs\": int(r.dropna().shape[0]),\n",
    "    }\n",
    "\n",
    "# =======================================================\n",
    "# Tear sheet\n",
    "# =======================================================\n",
    "\n",
    "def _compute_ic_spearman(\n",
    "    signal: pd.DataFrame,\n",
    "    ret_fwd: pd.DataFrame,\n",
    "    dates: pd.Index,\n",
    "    min_assets: int,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Daily (rebalance-date) Spearman IC, cross-sectional.\"\"\"\n",
    "    # Align on dates first\n",
    "    signal = signal.reindex(index=ret_fwd.index)\n",
    "    dates = pd.Index(dates).intersection(signal.index).intersection(ret_fwd.index)\n",
    "\n",
    "    ic_vals = []\n",
    "    for dt in dates:\n",
    "        x = signal.loc[dt]\n",
    "        y = ret_fwd.loc[dt]\n",
    "        m = x.notna() & y.notna()\n",
    "        if m.sum() < min_assets:\n",
    "            ic_vals.append(np.nan)\n",
    "        else:\n",
    "            ic_vals.append(x[m].corr(y[m], method=\"spearman\"))\n",
    "    return pd.Series(ic_vals, index=dates, name=\"IC_spearman\")\n",
    "\n",
    "\n",
    "def _compute_coverage_and_nvalid(\n",
    "    signal: pd.DataFrame,\n",
    "    ret_fwd: pd.DataFrame,\n",
    "    dates: pd.Index,\n",
    ") -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Coverage (%) and valid count on rebalance dates.\"\"\"\n",
    "    dates = pd.Index(dates).intersection(signal.index).intersection(ret_fwd.index)\n",
    "    universe_size = signal.shape[1]\n",
    "\n",
    "    cov = []\n",
    "    nvalid = []\n",
    "    for dt in dates:\n",
    "        m = signal.loc[dt].notna() & ret_fwd.loc[dt].notna()\n",
    "        n = int(m.sum())\n",
    "        nvalid.append(n)\n",
    "        cov.append(n / universe_size if universe_size > 0 else np.nan)\n",
    "\n",
    "    return (\n",
    "        pd.Series(cov, index=dates, name=\"coverage\"),\n",
    "        pd.Series(nvalid, index=dates, name=\"n_valid\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def _turnover_proxy_from_labels(\n",
    "    bucket_lbl: pd.DataFrame,\n",
    "    rebalance_dates: pd.Index,\n",
    "    bucket_k: int,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Turnover proxy for a given bucket: 1 - overlap_ratio of bucket membership.\n",
    "    Computed on rebalance dates using labels at those dates.\n",
    "    \"\"\"\n",
    "    dts = pd.Index(rebalance_dates).intersection(bucket_lbl.index)\n",
    "    prev_members = None\n",
    "    vals = []\n",
    "\n",
    "    for dt in dts:\n",
    "        lbl = bucket_lbl.loc[dt]\n",
    "        members = set(lbl.index[lbl == bucket_k])\n",
    "        if prev_members is None:\n",
    "            vals.append(np.nan)\n",
    "        else:\n",
    "            if len(members) == 0:\n",
    "                vals.append(np.nan)\n",
    "            else:\n",
    "                overlap = len(members.intersection(prev_members))\n",
    "                vals.append(1.0 - overlap / len(members))\n",
    "        prev_members = members\n",
    "\n",
    "    return pd.Series(vals, index=dts, name=f\"turnover_bucket_{bucket_k}\")\n",
    "\n",
    "\n",
    "def _equity_curve_from_returns(r: pd.Series) -> pd.Series:\n",
    "    \"\"\"Equity curve on the same index as r (no resampling).\"\"\"\n",
    "    r = r.dropna()\n",
    "    if len(r) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "    return (1.0 + r).cumprod()\n",
    "\n",
    "def _drawdown_from_equity(eq: pd.Series) -> pd.Series:\n",
    "    \"\"\"Drawdown series from equity curve.\"\"\"\n",
    "    if eq is None or len(eq) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "    peak = eq.cummax()\n",
    "    return eq / peak - 1.0\n",
    "\n",
    "def _yearly_returns_from_returns(r: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calendar-year compounded returns from periodic returns r.\n",
    "    Works for H-day returns too (compounds all observations within each year).\n",
    "    \"\"\"\n",
    "    r = r.dropna()\n",
    "    if len(r) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "    # group by calendar year and compound\n",
    "    return (1.0 + r).groupby(r.index.year).prod() - 1.0\n",
    "\n",
    "\n",
    "def _compute_benchmark_ret_fwd(\n",
    "    bench_price: pd.DataFrame | pd.Series,\n",
    "    H: int,\n",
    "    entry_mode: str,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return series aligned to formation date t:\n",
    "      ret_fwd[t] = Px[t+H+1] / Px[t+1] - 1\n",
    "    bench_price can be:\n",
    "      - Series of Close (or Open) already chosen, OR\n",
    "      - DataFrame with columns including 'Close' and/or 'Open'\n",
    "    \"\"\"\n",
    "    if isinstance(bench_price, pd.Series):\n",
    "        px = bench_price.sort_index()\n",
    "    else:\n",
    "        bench_price = bench_price.sort_index()\n",
    "        if entry_mode in {\"next_open\"}:\n",
    "            if \"Open\" not in bench_price.columns:\n",
    "                raise ValueError(\"benchmark DataFrame must contain 'Open' for entry_mode='next_open'\")\n",
    "            px = bench_price[\"Open\"]\n",
    "        else:\n",
    "            if \"Close\" not in bench_price.columns:\n",
    "                raise ValueError(\"benchmark DataFrame must contain 'Close' for entry_mode='next_close' or 'open_to_close'\")\n",
    "            px = bench_price[\"Close\"]\n",
    "\n",
    "    # open_to_close benchmark: enter open[t+1], exit close[t+H+1]\n",
    "    if entry_mode == \"open_to_close\":\n",
    "        if isinstance(bench_price, pd.Series):\n",
    "            raise ValueError(\"For entry_mode='open_to_close', benchmark must be a DataFrame with Open/Close.\")\n",
    "        entry_px = bench_price[\"Open\"].shift(-1)\n",
    "        exit_px  = bench_price[\"Close\"].shift(-(H + 1))\n",
    "        return (exit_px / entry_px - 1).rename(\"benchmark_ret_fwd\")\n",
    "\n",
    "    # next_close / next_open\n",
    "    entry_px = px.shift(-1)\n",
    "    exit_px  = px.shift(-(H + 1))\n",
    "    return (exit_px / entry_px - 1).rename(\"benchmark_ret_fwd\")\n",
    "\n",
    "\n",
    "def make_tearsheet(\n",
    "    price_df: pd.DataFrame,\n",
    "    signal: pd.DataFrame,\n",
    "    H: int = 5,\n",
    "    n_buckets: int = 20,\n",
    "    entry_mode: str = \"next_close\",\n",
    "    min_assets_ic: int = 50,\n",
    "    plot: bool = True,\n",
    "    rolling_window_obs: Optional[int] = None,\n",
    "    benchmark_price: Optional[pd.DataFrame | pd.Series] = None,\n",
    "    benchmark_name: str = \"Benchmark\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a standardized tear sheet for a signal under your bucket backtest framework.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    price_df : DataFrame\n",
    "        yfinance multiindex columns, must contain 'Close' and (if next_open/open_to_close) 'Open'\n",
    "    signal : DataFrame\n",
    "        dates x tickers, computed at close[t]\n",
    "    H : int\n",
    "        holding horizon (in trading days after entry), and rebalance step\n",
    "    n_buckets : int\n",
    "        number of buckets\n",
    "    entry_mode : str\n",
    "        'next_close' | 'next_open' | 'open_to_close'\n",
    "    min_assets_ic : int\n",
    "        minimum valid assets required to compute IC on a rebalance date\n",
    "    plot : bool\n",
    "        whether to display plots\n",
    "    rolling_window_obs : int | None\n",
    "        rolling window in number of rebalance observations; if None, defaults to ~1 year = 252/H\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    report : dict\n",
    "        Contains bucket_ret, ic series, summaries, coverage, turnover proxies, and summary tables.\n",
    "    \"\"\"\n",
    "    if rolling_window_obs is None:\n",
    "        rolling_window_obs = max(10, int(round(TRADING_DAYS / H)))\n",
    "\n",
    "    # Run backtest (uses strict NaN logic, next-day entry alignment)\n",
    "    bucket_ret, bucket_lbl, ret_fwd = bucket_backtest(\n",
    "        price_df=price_df,\n",
    "        signal=signal,\n",
    "        H=H,\n",
    "        n_buckets=n_buckets,\n",
    "        entry_mode=entry_mode,\n",
    "    )\n",
    "\n",
    "    rebalance_dates = bucket_ret.index\n",
    "    freq = TRADING_DAYS / H  # annualisation frequency for H-day non-overlapping returns\n",
    "\n",
    "    # Coverage & valid count\n",
    "    signal_aligned = signal.reindex(index=ret_fwd.index)\n",
    "    coverage, n_valid = _compute_coverage_and_nvalid(signal_aligned, ret_fwd, rebalance_dates)\n",
    "\n",
    "    # IC series + stats\n",
    "    ic = _compute_ic_spearman(signal_aligned, ret_fwd, rebalance_dates, min_assets=min_assets_ic)\n",
    "    ic_stats = {\n",
    "        \"ic_mean\": float(ic.mean()),\n",
    "        \"ic_std\": float(ic.std(ddof=1)),\n",
    "        \"icir\": float(ic.mean() / ic.std(ddof=1)) if ic.std(ddof=1) and np.isfinite(ic.std(ddof=1)) else np.nan,\n",
    "        \"hit_rate\": float((ic > 0).mean()),\n",
    "        \"n_obs\": int(ic.dropna().shape[0]),\n",
    "    }\n",
    "\n",
    "    ic_roll = ic.rolling(rolling_window_obs).mean()\n",
    "\n",
    "    # Bucket summary table\n",
    "    bucket_summary_rows = []\n",
    "    for col in bucket_ret.columns:\n",
    "        r = bucket_ret[col].dropna()\n",
    "        mu = float(r.mean()) if len(r) else np.nan\n",
    "        sd = float(r.std(ddof=1)) if len(r) >= 2 else np.nan\n",
    "        tstat = (mu / (sd / np.sqrt(len(r)))) if (len(r) >= 2 and sd > 0) else np.nan\n",
    "        ps = perf_summary(bucket_ret[col], freq=freq)\n",
    "        bucket_summary_rows.append({\n",
    "            \"bucket\": col,\n",
    "            \"mean_ret_per_period\": mu,\n",
    "            \"std_ret_per_period\": sd,\n",
    "            \"t_stat_mean\": tstat,\n",
    "            **ps,\n",
    "        })\n",
    "    bucket_summary = pd.DataFrame(bucket_summary_rows).set_index(\"bucket\")\n",
    "\n",
    "    # Monotonicity diagnostic: correlation between bucket number and mean return\n",
    "    # (Simple, fast sanity check)\n",
    "    mean_by_bucket = bucket_summary[\"mean_ret_per_period\"].copy()\n",
    "    tmp = mean_by_bucket.copy()\n",
    "    tmp.index = tmp.index.str.replace(\"bucket_\", \"\", regex=False).astype(int)\n",
    "    tmp = tmp.sort_index()  # 1..n_buckets\n",
    "    monotonic_spearman = float(\n",
    "        pd.Series(tmp.index, index=tmp.index).corr(tmp, method=\"spearman\")\n",
    "    )\n",
    "\n",
    "\n",
    "    # Turnover proxies for bottom and top buckets\n",
    "    turnover_bottom = _turnover_proxy_from_labels(bucket_lbl, rebalance_dates, bucket_k=1)\n",
    "    turnover_top = _turnover_proxy_from_labels(bucket_lbl, rebalance_dates, bucket_k=n_buckets)\n",
    "    \n",
    "    \n",
    "    bench_ret = None\n",
    "    bench_eq = None\n",
    "    bench_dd = None\n",
    "    bench_yearly = None\n",
    "    \n",
    "    if benchmark_price is not None:\n",
    "        bench_ret_full = _compute_benchmark_ret_fwd(benchmark_price, H=H, entry_mode=entry_mode)\n",
    "    \n",
    "        # Align to rebalance dates (because bucket_ret is on rebalance dates)\n",
    "        bench_ret = bench_ret_full.reindex(rebalance_dates)\n",
    "    \n",
    "        # Equity / DD / yearly on the same rebalance grid\n",
    "        bench_eq = _equity_curve_from_returns(bench_ret)\n",
    "        bench_dd = _drawdown_from_equity(bench_eq)\n",
    "        bench_yearly = _yearly_returns_from_returns(bench_ret).rename(benchmark_name)\n",
    "\n",
    "\n",
    "    # Optional plots\n",
    "    if plot:\n",
    "        # # 1) Coverage\n",
    "        # plt.figure()\n",
    "        # coverage.plot()\n",
    "        # plt.title(f\"Coverage on Rebalance Dates (H={H}, entry={entry_mode})\")\n",
    "        # plt.ylabel(\"Coverage (valid fraction)\")\n",
    "        # plt.xlabel(\"Date\")\n",
    "        # plt.show()\n",
    "\n",
    "        # 2) Cumulative IC + rolling IC\n",
    "        plt.figure()\n",
    "        ic.cumsum().plot()\n",
    "        plt.title(\"Cumulative IC (Spearman)\")\n",
    "        plt.ylabel(\"Cumulative IC\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        ic_roll.plot()\n",
    "        plt.title(f\"Rolling Mean IC (window={rolling_window_obs} obs)\")\n",
    "        plt.ylabel(\"Rolling IC mean\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.show()\n",
    "\n",
    "        # 3) Bucket mean returns bar chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        mean_by_bucket.sort_index(key=lambda idx: idx.str.replace(\"bucket_\", \"\", regex=False).astype(int)).plot(kind=\"bar\")\n",
    "        plt.title(\"Mean Return per Bucket (per holding period)\")\n",
    "        plt.ylabel(\"Mean H-day return\")\n",
    "        plt.xlabel(\"Bucket\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 4) Bucket cumulative curves (all buckets; if too busy, you can slice)\n",
    "        plt.figure()\n",
    "        # Build equity curves from bucket returns\n",
    "        eq = (1.0 + bucket_ret.fillna(0.0)).cumprod()\n",
    "        eq.plot(legend=True)\n",
    "        plt.legend()\n",
    "        plt.title(\"Bucket Cumulative Curves\")\n",
    "        plt.ylabel(\"Cumulative growth\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # # 5) Turnover proxy (top/bottom)\n",
    "        # plt.figure()\n",
    "        # pd.DataFrame({\"turnover_bottom\": turnover_bottom, \"turnover_top\": turnover_top}).plot()\n",
    "        # plt.title(\"Turnover Proxy (Bucket Membership Change)\")\n",
    "        # plt.ylabel(\"1 - overlap ratio\")\n",
    "        # plt.xlabel(\"Date\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        \n",
    "        # 6) Drawdown\n",
    "        # Select series to plot drawdown/yearly returns for\n",
    "        top_col = \"bucket_1\"\n",
    "        bot_col = f\"bucket_{n_buckets}\"\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        # top bucket\n",
    "        eq_top = _equity_curve_from_returns(bucket_ret[top_col])\n",
    "        dd_top = _drawdown_from_equity(eq_top)\n",
    "        if len(dd_top):\n",
    "            dd_top.plot(label=f\"{top_col} (max_dd={dd_top.min():.2%})\")\n",
    "        \n",
    "        # bottom bucket\n",
    "        eq_bot = _equity_curve_from_returns(bucket_ret[bot_col])\n",
    "        dd_bot = _drawdown_from_equity(eq_bot)\n",
    "        if len(dd_bot):\n",
    "            dd_bot.plot(label=f\"{bot_col} (max_dd={dd_bot.min():.2%})\")\n",
    "        \n",
    "        # benchmark\n",
    "        if bench_dd is not None and len(bench_dd):\n",
    "            bench_dd.plot(label=f\"{benchmark_name} (max_dd={bench_dd.min():.2%})\")\n",
    "        \n",
    "        plt.title(\"Drawdown Curves (Top / Bottom / Benchmark)\")\n",
    "        plt.ylabel(\"Drawdown\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 7) Yearly Returns\n",
    "        yr_top = _yearly_returns_from_returns(bucket_ret[top_col]).rename(top_col)\n",
    "        yr_bot = _yearly_returns_from_returns(bucket_ret[bot_col]).rename(bot_col)\n",
    "        \n",
    "        parts = [yr_top, yr_bot]\n",
    "        if bench_yearly is not None and len(bench_yearly):\n",
    "            parts.append(bench_yearly)\n",
    "        \n",
    "        yr_tbl = pd.concat(parts, axis=1).sort_index()\n",
    "        \n",
    "        if len(yr_tbl):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            yr_tbl.plot(kind=\"bar\")\n",
    "            plt.title(\"Calendar-Year Returns (Top / Bottom / Benchmark)\")\n",
    "            plt.ylabel(\"Return\")\n",
    "            plt.xlabel(\"Year\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    report = {\n",
    "        \"meta\": {\n",
    "            \"H\": H,\n",
    "            \"n_buckets\": n_buckets,\n",
    "            \"entry_mode\": entry_mode,\n",
    "            \"freq\": freq,\n",
    "            \"rolling_window_obs\": rolling_window_obs,\n",
    "            \"universe_size\": int(signal.shape[1]),\n",
    "            \"n_rebalance_obs\": int(len(rebalance_dates)),\n",
    "        },\n",
    "        \"bucket_ret\": bucket_ret,\n",
    "        \"benchmark_ret\": bench_ret,\n",
    "        \"bucket_labels\": bucket_lbl,\n",
    "        \"ret_fwd\": ret_fwd,\n",
    "        \"coverage\": coverage,\n",
    "        \"n_valid\": n_valid,\n",
    "        \"ic\": ic,\n",
    "        \"ic_stats\": ic_stats,\n",
    "        \"bucket_summary\": bucket_summary,\n",
    "        \"monotonic_spearman_bucket_vs_mean\": monotonic_spearman,\n",
    "        \"turnover_bottom\": turnover_bottom,\n",
    "        \"turnover_top\": turnover_top,\n",
    "    }\n",
    "    return report"
   ],
   "id": "2dc22ffc8c795ae5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "n_buckets = 10",
   "id": "4e231a09114ac8b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "signal = build_ma_signal(price_df, windows=(5, 20, 50, 120))\n",
    "\n",
    "rep = make_tearsheet(\n",
    "    price_df=price_df,\n",
    "    signal=signal,\n",
    "    H=5,\n",
    "    n_buckets=n_buckets,\n",
    "    entry_mode=\"next_open\",\n",
    "    min_assets_ic=50,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Key tables/series:\n",
    "display(rep[\"ic_stats\"])\n",
    "display(rep[\"bucket_summary\"])\n",
    "display(rep[\"coverage\"].describe())\n"
   ],
   "id": "dd8f13f7bf6e1d2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rsi = build_rsi_signal(price_df, period=14)\n",
    "macd = build_macd_signal(price_df, fast=12, slow=26, signal=9, output=\"hist\")\n",
    "\n",
    "# Hygiene\n",
    "rsi_z = cs_winsorize_zscore(rsi)\n",
    "macd_z = cs_winsorize_zscore(macd)"
   ],
   "id": "ad9cb9894274e33b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rep_rsi = make_tearsheet(\n",
    "    price_df=price_df,\n",
    "    signal=rsi_z,\n",
    "    H=10,\n",
    "    n_buckets=n_buckets,\n",
    "    entry_mode=\"next_open\",   # signal at close[t], enter close[t+1]\n",
    "    min_assets_ic=50,\n",
    "    plot=True                  # shows coverage, IC, bucket mean bar, bucket curves, turnover\n",
    ")\n",
    "\n",
    "display(rep_rsi[\"ic_stats\"])\n",
    "display(rep_rsi[\"bucket_summary\"])\n",
    "display(rep_rsi[\"coverage\"].describe())\n"
   ],
   "id": "bec6eefc68825349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "signal = build_ma_signal(price_df, windows=(5, 20))\n",
    "\n",
    "rep = make_tearsheet(\n",
    "    price_df=price_df,\n",
    "    signal=signal,\n",
    "    H=2,\n",
    "    n_buckets=n_buckets,\n",
    "    entry_mode=\"next_open\",\n",
    "    min_assets_ic=50,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Key tables/series:\n",
    "display(rep[\"ic_stats\"])\n",
    "display(rep[\"bucket_summary\"])\n",
    "display(rep[\"coverage\"].describe())\n"
   ],
   "id": "b56e19a5d02737a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bucket_returns = rep['bucket_ret'].copy()\n",
    "bucket_returns = bucket_returns.merge(benchmark_close.pct_change(), how='inner', left_index=True, right_index=True)\n",
    "bucket_returns.corr()"
   ],
   "id": "f8bad8c4233b41d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "signal = build_ma_signal(price_df, windows=(5,))\n",
    "\n",
    "rep = make_tearsheet(\n",
    "    price_df=price_df,\n",
    "    signal=signal,\n",
    "    H=1,\n",
    "    n_buckets=n_buckets,\n",
    "    entry_mode=\"next_open\",\n",
    "    min_assets_ic=50,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Key tables/series:\n",
    "display(rep[\"ic_stats\"])\n",
    "display(rep[\"bucket_summary\"])\n",
    "display(rep[\"coverage\"].describe())\n"
   ],
   "id": "bbf90f4805e2a9c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_past_vol_signal(\n",
    "    price_df: pd.DataFrame,\n",
    "    window: int = 20,\n",
    "    use_log_returns: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Past volatility signal: rolling std of daily returns.\n",
    "    Output is a DataFrame (dates x tickers) of *past* vol at each date t.\n",
    "\n",
    "    - If use_log_returns=True: vol of log returns (more stable)\n",
    "    - window=20: ~1 month\n",
    "    \"\"\"\n",
    "    close = price_df[\"Close\"].sort_index()\n",
    "\n",
    "    if use_log_returns:\n",
    "        ret = np.log(close).diff()\n",
    "    else:\n",
    "        ret = close.pct_change(fill_method=None)\n",
    "\n",
    "    vol = ret.rolling(window, min_periods=window).std(ddof=1)\n",
    "    return vol\n",
    "\n",
    "def cs_winsorize_zscore(sig: pd.DataFrame, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.DataFrame:\n",
    "    def _proc_row(x: pd.Series) -> pd.Series:\n",
    "        m = x.notna()\n",
    "        if m.sum() < 10:\n",
    "            return x\n",
    "        lo = x[m].quantile(lower_q)\n",
    "        hi = x[m].quantile(upper_q)\n",
    "        x2 = x.copy()\n",
    "        x2[m] = x[m].clip(lo, hi)\n",
    "        mu = x2[m].mean()\n",
    "        sd = x2[m].std(ddof=1)\n",
    "        if sd == 0 or not np.isfinite(sd):\n",
    "            return x2\n",
    "        x2[m] = (x2[m] - mu) / sd\n",
    "        return x2\n",
    "    return sig.apply(_proc_row, axis=1)\n",
    "\n",
    "vol = build_past_vol_signal(price_df, window=40, use_log_returns=True)\n",
    "vol_z = cs_winsorize_zscore(vol)\n",
    "\n",
    "signal_lowvol = -vol_z  # low vol => high score\n",
    "\n",
    "rep = make_tearsheet(\n",
    "    price_df=price_df,\n",
    "    signal=signal_lowvol,\n",
    "    H=5,\n",
    "    n_buckets=n_buckets,\n",
    "    entry_mode=\"next_open\",\n",
    "    min_assets_ic=50,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(rep[\"ic_stats\"])\n",
    "print(rep[\"bucket_summary\"].tail())  # top buckets if monotonic"
   ],
   "id": "156d6262ad216d1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6f3a7ed49e0906b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def bucket_constituents(\n",
    "    bucket_lbl: pd.DataFrame,\n",
    "    date,\n",
    "    k: int,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns list of tickers in bucket k on a given date.\n",
    "    bucket_lbl is typically forward-filled in your backtest.\n",
    "    \"\"\"\n",
    "    d = pd.to_datetime(date)\n",
    "    if d not in bucket_lbl.index:\n",
    "        # nearest previous date\n",
    "        d = bucket_lbl.index[bucket_lbl.index.get_indexer([d], method=\"ffill\")[0]]\n",
    "    members = bucket_lbl.loc[d]\n",
    "    return members.index[(members == k).fillna(False)].tolist()\n",
    "\n",
    "def bucket_snapshot_table(\n",
    "    bucket_lbl: pd.DataFrame,\n",
    "    signal: pd.DataFrame,\n",
    "    ret_fwd: pd.DataFrame,\n",
    "    date,\n",
    "    k: int,\n",
    "    sort_by: str = \"signal\",\n",
    "    ascending: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a table for bucket k on date with:\n",
    "      - signal[t, i]\n",
    "      - forward return ret_fwd[t, i] (as used by backtest)\n",
    "      - bucket id\n",
    "    \"\"\"\n",
    "    d = pd.to_datetime(date)\n",
    "    if d not in bucket_lbl.index:\n",
    "        d = bucket_lbl.index[bucket_lbl.index.get_indexer([d], method=\"ffill\")[0]]\n",
    "\n",
    "    members = bucket_lbl.loc[d]\n",
    "    tickers = members.index[(members == k).fillna(False)]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"bucket\": members.loc[tickers].astype(int),\n",
    "        \"signal\": signal.loc[d, tickers],\n",
    "        \"ret_fwd\": ret_fwd.loc[d, tickers],\n",
    "    }, index=tickers)\n",
    "\n",
    "    if sort_by in df.columns:\n",
    "        df = df.sort_values(sort_by, ascending=ascending)\n",
    "    return df\n",
    "\n",
    "def stock_bucket_history(\n",
    "    bucket_lbl: pd.DataFrame,\n",
    "    signal: pd.DataFrame,\n",
    "    ret_fwd: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    start=None,\n",
    "    end=None,\n",
    ") -> pd.DataFrame:\n",
    "    s = bucket_lbl[ticker].rename(\"bucket\")\n",
    "    out = pd.concat([\n",
    "        s,\n",
    "        signal[ticker].rename(\"signal\"),\n",
    "        ret_fwd[ticker].rename(\"ret_fwd\"),\n",
    "    ], axis=1)\n",
    "\n",
    "    if start is not None:\n",
    "        out = out.loc[pd.to_datetime(start):]\n",
    "    if end is not None:\n",
    "        out = out.loc[:pd.to_datetime(end)]\n",
    "    return out\n",
    "\n",
    "def stock_conditional_performance(\n",
    "    bucket_lbl: pd.DataFrame,\n",
    "    ret_fwd: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    k: int,\n",
    "    start=None,\n",
    "    end=None,\n",
    ") -> dict:\n",
    "    s_bucket = bucket_lbl[ticker]\n",
    "    s_ret = ret_fwd[ticker]\n",
    "\n",
    "    df = pd.concat([s_bucket.rename(\"bucket\"), s_ret.rename(\"ret_fwd\")], axis=1)\n",
    "    if start is not None:\n",
    "        df = df.loc[pd.to_datetime(start):]\n",
    "    if end is not None:\n",
    "        df = df.loc[:pd.to_datetime(end)]\n",
    "\n",
    "    cond = df.loc[df[\"bucket\"] == k, \"ret_fwd\"].dropna()\n",
    "\n",
    "    if len(cond) == 0:\n",
    "        return {\"ticker\": ticker, \"bucket\": k, \"n\": 0, \"mean\": np.nan, \"winrate\": np.nan}\n",
    "\n",
    "    return {\n",
    "        \"ticker\": ticker,\n",
    "        \"bucket\": k,\n",
    "        \"n\": int(cond.shape[0]),\n",
    "        \"mean\": float(cond.mean()),\n",
    "        \"winrate\": float((cond > 0).mean()),\n",
    "        \"std\": float(cond.std(ddof=1)) if cond.shape[0] > 1 else np.nan,\n",
    "    }\n",
    "\n",
    "def bucket_membership_frequency(\n",
    "    bucket_lbl: pd.DataFrame,\n",
    "    k: int,\n",
    "    start=None,\n",
    "    end=None,\n",
    ") -> pd.Series:\n",
    "    df = bucket_lbl.copy()\n",
    "    if start is not None:\n",
    "        df = df.loc[pd.to_datetime(start):]\n",
    "    if end is not None:\n",
    "        df = df.loc[:pd.to_datetime(end)]\n",
    "\n",
    "    freq = (df == k).sum(axis=0)  # counts of days in bucket k\n",
    "    freq = freq.sort_values(ascending=False)\n",
    "    return freq"
   ],
   "id": "a85ff6fa93e358e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bucket_lbl = rep['bucket_labels']\n",
    "ret_next = rep['ret_fwd']\n",
    "\n",
    "d = \"2023-06-30\"\n",
    "names = bucket_constituents(bucket_lbl, d, k=10)\n",
    "names"
   ],
   "id": "6e0a356a9c0035ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "snap = bucket_snapshot_table(bucket_lbl, signal, ret_next, d, k=1)\n",
    "snap"
   ],
   "id": "6a96a86b51e0299a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bucket_snapshot_table(bucket_lbl, signal, ret_next, d, k=10)",
   "id": "ceac28faa335027c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "signal_lowvol.tail()",
   "id": "e1ceab27dd91257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t = rep[\"bucket_ret\"].index[0]  # first rebalance date\n",
    "lbl_t = rep[\"bucket_labels\"].loc[t]\n",
    "sig_t = signal_lowvol.loc[t]\n",
    "\n",
    "# Compare bucket 1 vs bucket 10 signal distributions\n",
    "b1 = sig_t[lbl_t == 1].dropna()\n",
    "b10 = sig_t[lbl_t == 10].dropna()\n",
    "\n",
    "print(\"bucket1 median signal:\", b1.median(), \"n=\", len(b1))\n",
    "print(\"bucket10 median signal:\", b10.median(), \"n=\", len(b10))\n"
   ],
   "id": "4a35a77a3577a568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1caaff3df524484a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "749c733531f14b20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "43911cb1fcede060",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
