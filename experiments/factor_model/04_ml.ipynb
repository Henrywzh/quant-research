{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Iterable, Literal, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qresearch.data.io import load_hsci_2008_components, load_hsci_components_history\n",
    "from qresearch.data.utils import get_processed_dir, save_market_data_to_csv, load_market_data_from_csv\n",
    "from qresearch.data.yfinance import download_market_data"
   ],
   "id": "fa7784425dd5eab3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:06.745021Z",
     "start_time": "2026-01-19T21:54:06.486486Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# 0) Core utilities: naming + label/feature building\n",
    "# ============================================================\n",
    "def ensure_names(close_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    close = close_df.sort_index().copy()\n",
    "    close.index = pd.to_datetime(close.index)\n",
    "    close.index.name = \"date\"\n",
    "    close.columns.name = \"ticker\"\n",
    "    return close\n",
    "\n",
    "\n",
    "def make_fwd_return(close_df: pd.DataFrame, H: int = 5) -> pd.DataFrame:\n",
    "    close = ensure_names(close_df)\n",
    "    return close.shift(-H) / close - 1.0\n",
    "\n",
    "\n",
    "def rsi_wilder(close: pd.DataFrame, period: int = 14) -> pd.DataFrame:\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0.0)\n",
    "    down = (-delta).clip(lower=0.0)\n",
    "    roll_up = up.ewm(alpha=1 / period, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1 / period, adjust=False).mean()\n",
    "    rs = roll_up / roll_down\n",
    "    return 100.0 - (100.0 / (1.0 + rs))\n",
    "\n",
    "\n",
    "def make_features_default(close_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    close = ensure_names(close_df)\n",
    "\n",
    "    ma5 = close.rolling(5).mean()\n",
    "    ma10 = close.rolling(10).mean()\n",
    "    ma20 = close.rolling(20).mean()\n",
    "\n",
    "    close_ma_diff_5 = close / ma5 - 1.0\n",
    "    close_ma_diff_10 = close / ma10 - 1.0\n",
    "    close_ma_diff_20 = close / ma20 - 1.0\n",
    "\n",
    "    max_growth_5 = close / close.shift(5) - 1.0\n",
    "\n",
    "    rsi = rsi_wilder(close, period=14)\n",
    "\n",
    "    roll_max_120 = close.rolling(120).max()\n",
    "    dd = close / roll_max_120 - 1.0\n",
    "    max_drawdown_120 = dd.rolling(120).min()\n",
    "\n",
    "    return {\n",
    "        \"close_ma_diff_5\": close_ma_diff_5,\n",
    "        \"max_growth_5\": max_growth_5,\n",
    "        \"close_ma_diff_10\": close_ma_diff_10,\n",
    "        \"rsi\": rsi,\n",
    "        \"close_ma_diff_20\": close_ma_diff_20,\n",
    "        \"max_drawdown_120\": max_drawdown_120,\n",
    "    }\n",
    "\n",
    "\n",
    "def wide_to_panel(features: Dict[str, pd.DataFrame], y_wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert wide feature dict + wide y into a long panel:\n",
    "    columns: date, ticker, <feature...>, y\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for name, df in features.items():\n",
    "        s = df.stack(future_stack=True).rename(name)\n",
    "        parts.append(s)\n",
    "\n",
    "    X = pd.concat(parts, axis=1)\n",
    "    y = y_wide.stack(future_stack=True).rename(\"y\")\n",
    "\n",
    "    panel = pd.concat([X, y], axis=1)\n",
    "    panel.index = panel.index.set_names([\"date\", \"ticker\"])\n",
    "    panel = panel.reset_index()\n",
    "    panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
    "    return panel\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Cross-sectional preprocessing\n",
    "# ============================================================\n",
    "def cs_winsorize_zscore(panel: pd.DataFrame, feature_cols: list[str], q_lo: float = 0.01, q_hi: float = 0.99) -> pd.DataFrame:\n",
    "    out = panel.copy()\n",
    "    for c in feature_cols:\n",
    "        g = out.groupby(\"date\")[c]\n",
    "        lo = g.transform(lambda s: s.quantile(q_lo))\n",
    "        hi = g.transform(lambda s: s.quantile(q_hi))\n",
    "        x = out[c].clip(lo, hi)\n",
    "        mu = x.groupby(out[\"date\"]).transform(\"mean\")\n",
    "        sd = x.groupby(out[\"date\"]).transform(\"std\")\n",
    "        out[c] = (x - mu) / sd\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def flip_negative_ic(panel: pd.DataFrame, feature_cols: list[str]) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Pooled Spearman IC between each feature and y.\n",
    "    If IC < 0, flip sign to enforce positive direction.\n",
    "    \"\"\"\n",
    "    out = panel.copy()\n",
    "    ic_map: Dict[str, float] = {}\n",
    "    for c in feature_cols:\n",
    "        ic = out[c].corr(out[\"y\"], method=\"spearman\")\n",
    "        ic_map[c] = float(ic) if ic is not None else np.nan\n",
    "        if ic is not None and ic < 0:\n",
    "            out[c] = -out[c]\n",
    "    return out, ic_map\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Walk-forward split (2y train / 1m test + purge H)\n",
    "# ============================================================\n",
    "def walk_forward_splits(\n",
    "    dates: Iterable[pd.Timestamp],\n",
    "    train_years: int = 2,\n",
    "    test_months: int = 1,\n",
    "    H: int = 5,\n",
    "    min_train_days: int = 100,\n",
    "    min_test_days: int = 5,\n",
    "):\n",
    "    dates = pd.DatetimeIndex(sorted(pd.DatetimeIndex(dates).unique()))\n",
    "    start = dates.min()\n",
    "    end = dates.max()\n",
    "\n",
    "    cur_test_start = start + pd.DateOffset(years=train_years)\n",
    "    while cur_test_start < end:\n",
    "        cur_test_end = cur_test_start + pd.DateOffset(months=test_months)\n",
    "\n",
    "        test_dates = dates[(dates >= cur_test_start) & (dates < cur_test_end)]\n",
    "        if len(test_dates) == 0:\n",
    "            cur_test_start = cur_test_end\n",
    "            continue\n",
    "\n",
    "        test_start = test_dates.min()\n",
    "        pos = dates.get_indexer([test_start])[0]\n",
    "        train_end_pos = max(pos - H, 0)  # purge H days\n",
    "        train_end = dates[train_end_pos]\n",
    "\n",
    "        train_start = test_start - pd.DateOffset(years=train_years)\n",
    "        train_dates = dates[(dates >= train_start) & (dates <= train_end)]\n",
    "\n",
    "        if len(train_dates) >= min_train_days and len(test_dates) >= min_test_days:\n",
    "            yield train_dates, test_dates\n",
    "\n",
    "        cur_test_start = cur_test_end\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Model adapters (single interface)\n",
    "# ============================================================\n",
    "@dataclass(frozen=True)\n",
    "class FitPredictContext:\n",
    "    feature_cols: list[str]\n",
    "    top_k: int\n",
    "    H: int\n",
    "\n",
    "\n",
    "FitPredictFn = Callable[\n",
    "    [pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, np.ndarray, FitPredictContext, dict],\n",
    "    np.ndarray\n",
    "]\n",
    "\"\"\"\n",
    "Signature:\n",
    "    predict = fit_predict(\n",
    "        train_df, test_df,\n",
    "        X_train, y_train, group_train,\n",
    "        X_test, group_test,\n",
    "        ctx, params\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "def make_relevance_from_y(\n",
    "    panel: pd.DataFrame,\n",
    "    n_bins: int = 5,\n",
    "    col_y: str = \"y\",\n",
    "    col_date: str = \"date\",\n",
    "    min_per_day: int = 20,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert continuous forward returns y into integer relevance labels per day.\n",
    "\n",
    "    Why:\n",
    "    - LightGBM LambdaRank expects integer labels (relevance grades).\n",
    "    - Relevance must be cross-sectional per query group (here: per date).\n",
    "\n",
    "    Robustness:\n",
    "    - Handles ties / insufficient unique values / small universes.\n",
    "    - Guarantees output is integer in [0, n_bins-1] where defined.\n",
    "    - Returns pd.Series aligned to panel.index (same length).\n",
    "\n",
    "    Convention:\n",
    "    - Higher y => higher relevance (better).\n",
    "    \"\"\"\n",
    "    if n_bins < 2:\n",
    "        raise ValueError(\"n_bins must be >= 2\")\n",
    "\n",
    "    y = panel[col_y]\n",
    "    d = panel[col_date]\n",
    "\n",
    "    def _bin_one_day(s: pd.Series) -> pd.Series:\n",
    "        # s is y for one date, indexed by panel index rows\n",
    "        s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        # Not enough names to create stable bins\n",
    "        if len(s) < max(min_per_day, n_bins):\n",
    "            return pd.Series(index=s.index, data=np.nan)\n",
    "\n",
    "        # If too many ties (e.g., many zeros), qcut can fail or drop bins\n",
    "        # Strategy: rank then map ranks into bins deterministically.\n",
    "        r = s.rank(method=\"average\")  # ascending rank: low y -> small rank\n",
    "        # Convert rank percentiles to bins\n",
    "        # bin_id in 0..n_bins-1\n",
    "        bin_id = np.floor((r - 1) / max(len(r) / n_bins, 1.0))\n",
    "        bin_id = bin_id.clip(0, n_bins - 1)\n",
    "\n",
    "        # Ensure integer dtype but allow NaN at higher level; return Int64 for safety\n",
    "        return pd.Series(index=s.index, data=bin_id.astype(np.int64))\n",
    "\n",
    "    rel = panel.groupby(d, sort=False)[col_y].apply(_bin_one_day)\n",
    "    # groupby/apply returns a multiindex series; drop the group level so it aligns with original rows\n",
    "    rel = rel.reset_index(level=0, drop=True)\n",
    "\n",
    "    # Align to full panel index: rows that were NaN/dropped become NaN here\n",
    "    rel = rel.reindex(panel.index)\n",
    "\n",
    "    # Use pandas nullable Int64 to avoid int-casting NaN error.\n",
    "    return rel.astype(\"Int64\")\n",
    "\n",
    "\n",
    "def fit_predict_xgb_ranker(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    group_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    group_test: np.ndarray,\n",
    "    ctx: FitPredictContext,\n",
    "    params: dict,\n",
    ") -> np.ndarray:\n",
    "    from xgboost import XGBRanker\n",
    "\n",
    "    # map friendly params -> XGBRanker init\n",
    "    model = XGBRanker(**params)\n",
    "    model.fit(X_train, y_train, group=group_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "\n",
    "def fit_predict_lgb_lambdarank(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    group_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    group_test: np.ndarray,\n",
    "    ctx: FitPredictContext,\n",
    "    params: dict,\n",
    ") -> np.ndarray:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    # LightGBM needs Dataset + group\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train, group=group_train, free_raw_data=True)\n",
    "    dvalid = lgb.Dataset(X_test,  label=np.zeros_like(X_test[:, 0]), group=group_test, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "    # training params\n",
    "    lgb_params = params.get(\"lgb_params\", {})\n",
    "    num_boost_round = params.get(\"num_boost_round\", 2000)\n",
    "    early_stopping_rounds = params.get(\"early_stopping_rounds\", 50)\n",
    "\n",
    "    # ensure objective/metric exist\n",
    "    lgb_params = {\n",
    "        \"objective\": \"lambdarank\",\n",
    "        \"metric\": \"ndcg\",\n",
    "        \"verbosity\": -1,\n",
    "        \"ndcg_eval_at\": [ctx.top_k],\n",
    "        **lgb_params,\n",
    "    }\n",
    "\n",
    "    booster = lgb.train(\n",
    "        lgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[dvalid],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False)],\n",
    "    )\n",
    "    return booster.predict(X_test, num_iteration=booster.best_iteration)\n",
    "\n",
    "\n",
    "MODEL_REGISTRY: Dict[str, FitPredictFn] = {\n",
    "    \"xgb_ranker\": fit_predict_xgb_ranker,\n",
    "    \"lgb_lambdarank\": fit_predict_lgb_lambdarank,\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Shared evaluation\n",
    "# ============================================================\n",
    "def eval_rankic(oos: pd.DataFrame) -> Dict[str, object]:\n",
    "    rankic_by_day = oos.groupby(\"date\", group_keys=False)[[\"score\", \"y\"]].apply(lambda g: g[\"score\"].corr(g[\"y\"], method=\"spearman\"))\n",
    "    mean = float(rankic_by_day.mean())\n",
    "    std = float(rankic_by_day.std(ddof=0))\n",
    "    ir = float(mean / std) if std > 0 else np.nan\n",
    "    return {\"rankic_by_day\": rankic_by_day, \"rankic_mean\": mean, \"rankic_ir\": ir}\n",
    "\n",
    "\n",
    "def eval_topk_proxy(oos: pd.DataFrame, top_k: int) -> Dict[str, object]:\n",
    "    # Diagnostic only: mean of forward H-day return y among top-k scores\n",
    "    topk_fwd = oos.groupby(\"date\", group_keys=False)[[\"score\", \"y\"]].apply(lambda g: g.nlargest(top_k, \"score\")[\"y\"].mean())\n",
    "    topk_eq = (1 + topk_fwd.fillna(0)).cumprod()\n",
    "    return {\"topk_fwd\": topk_fwd, \"topk_fwd_eq\": topk_eq}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) The unified workflow (minimal duplication)\n",
    "# ============================================================\n",
    "def run_rank_workflow(\n",
    "    close_df: pd.DataFrame,\n",
    "    model_name: Literal[\"xgb_ranker\", \"lgb_lambdarank\"],\n",
    "    model_params: dict,\n",
    "    H: int = 5,\n",
    "    top_k: int = 7,\n",
    "    train_years: int = 2,\n",
    "    test_months: int = 1,\n",
    "    preprocess: bool = True,\n",
    "    align_feature_sign: bool = True,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Unified workflow for cross-sectional ranking models (walk-forward).\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    close_df:\n",
    "        Wide close price table: index=Date, columns=Ticker.\n",
    "        Features and y are computed using only information up to close[t].\n",
    "    model_name:\n",
    "        Which ranker to use (delegated to MODEL_REGISTRY).\n",
    "    model_params:\n",
    "        Model-specific parameters, plus optional workflow knobs like:\n",
    "          - relevance_bins (for lgb_lambdarank)\n",
    "    H:\n",
    "        Forward horizon: y[t] = fwd return over the holding window aligned to entry rule.\n",
    "    top_k:\n",
    "        Used by eval_topk_proxy (diagnostic), and can be used by the ranker objective.\n",
    "    train_years / test_months:\n",
    "        Walk-forward split definition.\n",
    "    preprocess:\n",
    "        Cross-sectional winsorize + zscore per date (recommended).\n",
    "    align_feature_sign:\n",
    "        Flip features with negative pooled IC so “higher is better” consistently.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "        - pooled_ic (optional)\n",
    "        - oos (long panel of predictions)\n",
    "        - oos_wide (date x ticker score table; directly usable as a signal)\n",
    "        - rankic stats\n",
    "        - topk proxy stats\n",
    "        - metadata\n",
    "    \"\"\"\n",
    "    if model_name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"Unknown model_name={model_name}. Available: {list(MODEL_REGISTRY.keys())}\")\n",
    "\n",
    "    fit_predict = MODEL_REGISTRY[model_name]\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Build a single shared panel: (date, ticker) rows with features + y\n",
    "    # -------------------------\n",
    "    feats = make_features_default(close_df)\n",
    "    y_wide = make_fwd_return(close_df, H=H)\n",
    "\n",
    "    panel = wide_to_panel(feats, y_wide)  # expects columns: date, ticker, <features...>, y\n",
    "\n",
    "    feature_cols = list(feats.keys())\n",
    "\n",
    "    # Hard clean y to avoid inf contaminating downstream ops\n",
    "    # (You can optionally extend to features, but you already dropna later.)\n",
    "    panel = panel[np.isfinite(panel[\"y\"].to_numpy())].copy()\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) LightGBM LambdaRank needs integer relevance grades per query group (date)\n",
    "    # -------------------------\n",
    "    if model_name == \"lgb_lambdarank\":\n",
    "        n_bins = int(model_params.get(\"relevance_bins\", 5))\n",
    "        panel = panel.copy()\n",
    "        panel[\"rel\"] = make_relevance_from_y(panel, n_bins=n_bins, col_y=\"y\", col_date=\"date\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Drop NA rows from rolling windows + end-of-sample horizon\n",
    "    # -------------------------\n",
    "    drop_cols = [\"y\"] + feature_cols\n",
    "    if model_name == \"lgb_lambdarank\":\n",
    "        # rel is allowed to be NA for bad dates; but training cannot.\n",
    "        drop_cols += [\"rel\"]\n",
    "\n",
    "    panel = panel.dropna(subset=drop_cols)\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Optional cross-sectional preprocessing (per date)\n",
    "    # -------------------------\n",
    "    if preprocess:\n",
    "        panel = cs_winsorize_zscore(panel, feature_cols)\n",
    "\n",
    "    pooled_ic = None\n",
    "    if align_feature_sign:\n",
    "        panel, pooled_ic = flip_negative_ic(panel, feature_cols)\n",
    "\n",
    "    # Ensure chronological order for splits\n",
    "    all_dates = pd.DatetimeIndex(panel[\"date\"].unique()).sort_values()\n",
    "\n",
    "    ctx = FitPredictContext(feature_cols=feature_cols, top_k=top_k, H=H)\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) Walk-forward: train -> predict -> collect OOS scores\n",
    "    # -------------------------\n",
    "    oos_rows = []\n",
    "\n",
    "    for train_dates, test_dates in walk_forward_splits(\n",
    "        all_dates,\n",
    "        train_years=train_years,\n",
    "        test_months=test_months,\n",
    "        H=H,\n",
    "    ):\n",
    "        train = panel[panel[\"date\"].isin(train_dates)].sort_values([\"date\", \"ticker\"])\n",
    "        test  = panel[panel[\"date\"].isin(test_dates)].sort_values([\"date\", \"ticker\"])\n",
    "\n",
    "        # Guard: skip empty splits (can happen if dates get filtered by NA drops)\n",
    "        if len(train) == 0 or len(test) == 0:\n",
    "            continue\n",
    "\n",
    "        # Group sizes: one query group per date\n",
    "        group_train = train.groupby(\"date\").size().to_numpy()\n",
    "        group_test  = test.groupby(\"date\").size().to_numpy()\n",
    "\n",
    "        X_train = train[feature_cols].to_numpy()\n",
    "        X_test  = test[feature_cols].to_numpy()\n",
    "\n",
    "        # y is always continuous forward return (kept for evaluation)\n",
    "        y_test = test[\"y\"].to_numpy()\n",
    "\n",
    "        # For training target:\n",
    "        # - XGB ranker can train directly on continuous y (pairwise)\n",
    "        # - LGBM lambdarank typically expects relevance grades (int)\n",
    "        if model_name == \"lgb_lambdarank\":\n",
    "            y_train_model = train[\"rel\"].to_numpy()\n",
    "        else:\n",
    "            y_train_model = train[\"y\"].to_numpy()\n",
    "\n",
    "        score = fit_predict(\n",
    "            train, test,\n",
    "            X_train, y_train_model, group_train,\n",
    "            X_test, group_test,\n",
    "            ctx, model_params,\n",
    "        )\n",
    "\n",
    "        tmp = test[[\"date\", \"ticker\"]].copy()\n",
    "        tmp[\"score\"] = score\n",
    "        tmp[\"y\"] = y_test\n",
    "        oos_rows.append(tmp)\n",
    "\n",
    "    if not oos_rows:\n",
    "        raise RuntimeError(\"No OOS rows produced. Check splits/NA filtering/universe coverage.\")\n",
    "\n",
    "    oos = (\n",
    "        pd.concat(oos_rows, ignore_index=True)\n",
    "        .sort_values([\"date\", \"score\"], ascending=[True, False])\n",
    "    )\n",
    "\n",
    "    # Wide score table: convenient “signal” to plug into bucket backtest / tearsheet\n",
    "    oos_wide = oos.pivot(index=\"date\", columns=\"ticker\", values=\"score\").sort_index()\n",
    "\n",
    "    rank_metrics = eval_rankic(oos)\n",
    "    topk_metrics = eval_topk_proxy(oos, top_k=top_k)\n",
    "\n",
    "    return {\n",
    "        \"pooled_ic\": pooled_ic,\n",
    "        \"oos\": oos,\n",
    "        \"oos_wide\": oos_wide,\n",
    "        **rank_metrics,\n",
    "        **topk_metrics,\n",
    "        \"model_name\": model_name,\n",
    "        \"model_params\": model_params,\n",
    "        \"H\": H,\n",
    "        \"top_k\": top_k,\n",
    "        \"train_years\": train_years,\n",
    "        \"test_months\": test_months,\n",
    "        \"preprocess\": preprocess,\n",
    "        \"align_feature_sign\": align_feature_sign,\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:09.148677Z",
     "start_time": "2026-01-19T21:54:09.141515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yfinance as yf\n",
    "\n",
    "processed_dir = get_processed_dir()\n",
    "\n",
    "hsci_stocks = pd.read_csv(f'{processed_dir}/hsci_components.csv')\n",
    "\n",
    "START_DATE = \"2009-01-01\"\n",
    "END_DATE   = None\n",
    "\n",
    "def code_int_to_hk(code: int) -> str:\n",
    "    return f\"{int(code):04d}.HK\"\n",
    "\n",
    "UNIVERSE = hsci_stocks[\"Stock Code\"].apply(code_int_to_hk).tolist()\n",
    "\n",
    "# price_df = download_market_data(UNIVERSE, start=START_DATE, end=END_DATE)"
   ],
   "id": "9366504fff53b78a",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:11.012515Z",
     "start_time": "2026-01-19T21:54:11.010419Z"
    }
   },
   "cell_type": "code",
   "source": "# save_market_data_to_csv(price_df, f'{processed_dir}/hsci_ohlc.csv')",
   "id": "8e055b7ca7e70f4c",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:12.442430Z",
     "start_time": "2026-01-19T21:54:11.500952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hsci_md = load_market_data_from_csv(f'{processed_dir}/hsci_ohlc.csv')\n",
    "hsci_md.close.head()"
   ],
   "id": "cf5b54d486eb0659",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MarketData from /Users/henrywzh/Desktop/Quant/Research/data/processed/hsci_ohlc.csv...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ticker       0883.HK   0857.HK   1088.HK   0386.HK   1171.HK   1898.HK  \\\n",
       "Date                                                                     \n",
       "2009-01-02  2.767038  3.167000  5.170709  1.308784  0.736292  3.657679   \n",
       "2009-01-05  2.985776  3.386929  5.349010  1.387944  0.807467  3.959227   \n",
       "2009-01-06  2.989422  3.439713  5.473820  1.374751  0.830782  4.188180   \n",
       "2009-01-07  2.880052  3.276964  5.444103  1.324616  0.809921  3.953643   \n",
       "2009-01-08  2.686833  3.105419  5.259860  1.245456  0.742428  3.573914   \n",
       "\n",
       "Ticker       2883.HK  3668.HK  2386.HK   0639.HK  ...   1052.HK  9699.HK  \\\n",
       "Date                                              ...                      \n",
       "2009-01-02  4.749063      NaN      NaN  0.425364  ...  0.829630      NaN   \n",
       "2009-01-05  5.000485      NaN      NaN  0.434272  ...  0.854555      NaN   \n",
       "2009-01-06  4.853823      NaN      NaN  0.414229  ...  0.868798      NaN   \n",
       "2009-01-07  4.679226      NaN      NaN  0.412002  ...  0.858115      NaN   \n",
       "2009-01-08  4.211303      NaN      NaN  0.420910  ...  0.811827      NaN   \n",
       "\n",
       "Ticker      2510.HK  0636.HK  1341.HK  2169.HK  2570.HK  3677.HK  2582.HK  \\\n",
       "Date                                                                        \n",
       "2009-01-02      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2009-01-05      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2009-01-06      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2009-01-07      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2009-01-08      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "Ticker      1333.HK  \n",
       "Date                 \n",
       "2009-01-02      NaN  \n",
       "2009-01-05      NaN  \n",
       "2009-01-06      NaN  \n",
       "2009-01-07      NaN  \n",
       "2009-01-08      NaN  \n",
       "\n",
       "[5 rows x 508 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>0883.HK</th>\n",
       "      <th>0857.HK</th>\n",
       "      <th>1088.HK</th>\n",
       "      <th>0386.HK</th>\n",
       "      <th>1171.HK</th>\n",
       "      <th>1898.HK</th>\n",
       "      <th>2883.HK</th>\n",
       "      <th>3668.HK</th>\n",
       "      <th>2386.HK</th>\n",
       "      <th>0639.HK</th>\n",
       "      <th>...</th>\n",
       "      <th>1052.HK</th>\n",
       "      <th>9699.HK</th>\n",
       "      <th>2510.HK</th>\n",
       "      <th>0636.HK</th>\n",
       "      <th>1341.HK</th>\n",
       "      <th>2169.HK</th>\n",
       "      <th>2570.HK</th>\n",
       "      <th>3677.HK</th>\n",
       "      <th>2582.HK</th>\n",
       "      <th>1333.HK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>2.767038</td>\n",
       "      <td>3.167000</td>\n",
       "      <td>5.170709</td>\n",
       "      <td>1.308784</td>\n",
       "      <td>0.736292</td>\n",
       "      <td>3.657679</td>\n",
       "      <td>4.749063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.425364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>2.985776</td>\n",
       "      <td>3.386929</td>\n",
       "      <td>5.349010</td>\n",
       "      <td>1.387944</td>\n",
       "      <td>0.807467</td>\n",
       "      <td>3.959227</td>\n",
       "      <td>5.000485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.434272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>2.989422</td>\n",
       "      <td>3.439713</td>\n",
       "      <td>5.473820</td>\n",
       "      <td>1.374751</td>\n",
       "      <td>0.830782</td>\n",
       "      <td>4.188180</td>\n",
       "      <td>4.853823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.414229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>2.880052</td>\n",
       "      <td>3.276964</td>\n",
       "      <td>5.444103</td>\n",
       "      <td>1.324616</td>\n",
       "      <td>0.809921</td>\n",
       "      <td>3.953643</td>\n",
       "      <td>4.679226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.412002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-08</th>\n",
       "      <td>2.686833</td>\n",
       "      <td>3.105419</td>\n",
       "      <td>5.259860</td>\n",
       "      <td>1.245456</td>\n",
       "      <td>0.742428</td>\n",
       "      <td>3.573914</td>\n",
       "      <td>4.211303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 508 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:38.798491Z",
     "start_time": "2026-01-19T21:54:38.796704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hsi_md = download_market_data(\"^HSI\", START_DATE, END_DATE)\n",
    "# \n",
    "# # Extract close matrices\n",
    "# close = hsci_md.close.sort_index()\n",
    "# # close.to_csv(f'{grandparent}/data/processed/hsci_close.csv')\n",
    "# benchmark_close = hsi_md.close.sort_index()\n",
    "# \n",
    "# # Common calendar (optional but recommended)\n",
    "# common_dates = close.index.intersection(benchmark_close.index)\n",
    "# close_df = close.loc[common_dates]\n",
    "# benchmark_close = benchmark_close.loc[common_dates]"
   ],
   "id": "dae1acd4650b3075",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:40.118442Z",
     "start_time": "2026-01-19T21:54:40.116304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from qresearch.backtest.buckets import make_tearsheet\n",
    "# \n",
    "# lgb_params = {\n",
    "#   \"relevance_bins\": 5,\n",
    "#   \"lgb_params\": {\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"num_leaves\": 31,\n",
    "#     \"min_data_in_leaf\": 300,\n",
    "#     \"feature_fraction\": 0.8,\n",
    "#     \"bagging_fraction\": 0.8,\n",
    "#     \"bagging_freq\": 1,\n",
    "#     \"lambda_l2\": 2.0,\n",
    "#     \"min_gain_to_split\": 0.0,\n",
    "#   },\n",
    "#   \"num_boost_round\": 800,\n",
    "# }\n",
    "# \n",
    "# out = run_rank_workflow(\n",
    "#     close_df=close_df,\n",
    "#     model_name=\"lgb_lambdarank\",\n",
    "#     model_params=lgb_params,\n",
    "#     H=5,\n",
    "#     top_k=10,\n",
    "# )\n",
    "# \n",
    "# signal_ml = out[\"oos_wide\"]  # date x ticker model scores\n",
    "# \n",
    "# rep = make_tearsheet(\n",
    "#     price_df=price_df,\n",
    "#     signal=signal_ml,\n",
    "#     H=5,\n",
    "#     n_buckets=20,\n",
    "#     entry_mode=\"next_close\",\n",
    "#     benchmark_price=hsi,  # optional\n",
    "#     benchmark_name=\"^HSI\",\n",
    "# )"
   ],
   "id": "7342421d4a8ef2cd",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:41.778895Z",
     "start_time": "2026-01-19T21:54:41.773695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qresearch.signals.signals_sweep import *\n",
    "\n",
    "# H = 5\n",
    "\n",
    "test_ma_diff = [\n",
    "    ('ma_diff', {\"lookback\": 5, \"skip\": 0}),\n",
    "    ('ma_diff', {\"lookback\": 20, \"skip\": 0}),\n",
    "    ('ma_diff', {\"lookback\": 120, \"skip\": 0}),\n",
    "    ('ma_diff', {\"lookback\": 200, \"skip\": 0}),\n",
    "    ('ma_diff', {\"lookback\": 250, \"skip\": 0}),\n",
    "]\n",
    "\n",
    "test_mom_ret = [\n",
    "    (\"mom_ret\", {\"lookback\": 5, \"skip\": 0}),\n",
    "    (\"mom_ret\", {\"lookback\": 21, \"skip\": 0}),\n",
    "    (\"mom_ret\", {\"lookback\": 126, \"skip\": 0}),\n",
    "    (\"mom_12_1\", {\"lookback\": 252, \"skip\": 21}),\n",
    "]\n",
    "\n",
    "test_ohlc_mom = [\n",
    "    (\"on_minus_id\", {\"lookback\": 5}),\n",
    "    (\"overnight_mom\", {\"lookback\": 5}),\n",
    "    ('intraday_mom', {\"lookback\": 5}),\n",
    "    (\"on_minus_id\", {\"lookback\": 10}),\n",
    "    (\"overnight_mom\", {\"lookback\": 10}),\n",
    "    ('intraday_mom', {\"lookback\": 10}),\n",
    "    (\"on_minus_id\", {\"lookback\": 21}),\n",
    "    (\"overnight_mom\", {\"lookback\": 21}),\n",
    "    ('intraday_mom', {\"lookback\": 21}),\n",
    "    (\"on_minus_id\", {\"lookback\": 63}),\n",
    "    (\"overnight_mom\", {\"lookback\": 63}),\n",
    "    ('intraday_mom', {\"lookback\": 63}),\n",
    "    (\"on_minus_id\", {\"lookback\": 126}),\n",
    "    (\"overnight_mom\", {\"lookback\": 126}),\n",
    "    ('intraday_mom', {\"lookback\": 126}),\n",
    "    (\"on_minus_id\", {\"lookback\": 252}),\n",
    "    (\"overnight_mom\", {\"lookback\": 252}),\n",
    "    ('intraday_mom', {\"lookback\": 252}),\n",
    "]\n",
    "\n",
    "test_rsi = [\n",
    "    (\"rsi\", {\"lookback\": 5}),\n",
    "    (\"rsi\", {\"lookback\": 12}),\n",
    "    (\"rsi\", {\"lookback\": 24}),\n",
    "]\n",
    "\n",
    "test_trend_annret_r2 = [\n",
    "    (\"trend_annret_r2\", {\"lookback\": 126, \"ann_factor\": 252}),\n",
    "    (\"trend_annret_r2\", {\"lookback\": 252, \"ann_factor\": 252}),\n",
    "    (\"trend_annret_r2\", {\"lookback\": 252, \"ann_factor\": 252, 'skip': 21}),\n",
    "]\n",
    "\n",
    "tests = test_ma_diff + test_mom_ret + test_trend_annret_r2 + test_ohlc_mom + test_rsi"
   ],
   "id": "20b7479106b675ea",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:42.259753Z",
     "start_time": "2026-01-19T21:54:42.257984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cfg = SignalTestConfig(H=5, n_buckets=10, use_price_floor=True)\n",
    "# summary = sweep_signals(hsci_md, tests, cfg)\n",
    "# summary"
   ],
   "id": "e6e61632128c035",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:42.599643Z",
     "start_time": "2026-01-19T21:54:42.597685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cfg = SignalTestConfig(H=10, n_buckets=10, use_price_floor=True)\n",
    "# summary = sweep_signals(hsci_md, tests, cfg)\n",
    "# summary"
   ],
   "id": "974b386ad7f13898",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:43.343280Z",
     "start_time": "2026-01-19T21:54:43.341773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cfg = SignalTestConfig(H=20, n_buckets=10, use_price_floor=True)\n",
    "# summary = sweep_signals(hsci_md, tests, cfg)\n",
    "# summary"
   ],
   "id": "fc006e70cc3f9592",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Market Cap, Market Data",
   "id": "10e56d05e6f27c82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:55:00.238465Z",
     "start_time": "2026-01-19T21:55:00.211097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qresearch.universe.hsci import build_hsci_ticker_master\n",
    "\n",
    "all_df = load_hsci_components_history()\n",
    "hsci_2008_df = load_hsci_2008_components()\n",
    "\n",
    "hsci_ticker_master = build_hsci_ticker_master(all_df, hsci_2008_df)\n",
    "\n",
    "tickers = hsci_ticker_master['ticker']\n",
    "\n",
    "hsci_ticker_master"
   ],
   "id": "e1fc0c4833ae80a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      ticker  in_seed_2008  in_history_events\n",
       "0    0001.HK          True               True\n",
       "1    0002.HK          True              False\n",
       "2    0003.HK          True              False\n",
       "3    0004.HK          True               True\n",
       "4    0005.HK          True              False\n",
       "..       ...           ...                ...\n",
       "929  9993.HK         False               True\n",
       "930  9995.HK         False               True\n",
       "931  9996.HK         False               True\n",
       "932  9997.HK         False               True\n",
       "933  9999.HK         False               True\n",
       "\n",
       "[934 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>in_seed_2008</th>\n",
       "      <th>in_history_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>9993.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>9995.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>9996.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>9997.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>9999.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>934 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:56:06.231453Z",
     "start_time": "2026-01-19T21:55:32.461515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---- market data ----\n",
    "hsci_all_md = download_market_data(tickers, start=START_DATE, end=END_DATE)"
   ],
   "id": "dcfea2a08a8564b3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                       0%                       ]  2 of 934 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Initializing download for 934 tickers...\n",
      "    Date Range: 2009-01-01 -> Now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  934 of 934 completed\n",
      "\n",
      "65 Failed downloads:\n",
      "['0735.HK', '0569.HK', '1821.HK', '0729.HK', '0378.HK', '2868.HK', '3799.HK', '2662.HK', '1131.HK', '0208.HK', '0342.HK', '0665.HK', '0985.HK', '1337.HK', '0996.HK', '6837.HK', '0054.HK', '2148.HK', '0903.HK', '1175.HK', '0885.HK', '1366.HK', '0109.HK', '0973.HK', '1589.HK', '1297.HK', '3308.HK', '1573.HK', '1212.HK', '1363.HK', '1992.HK', '1169.HK', '2686.HK', '1558.HK', '1031.HK', '0530.HK', '1381.HK', '0043.HK', '0678.HK', '1089.HK', '1806.HK', '0190.HK', '0416.HK', '2103.HK', '0925.HK', '9997.HK', '0494.HK', '0555.HK', '1194.HK', '1829.HK', '0680.HK', '0273.HK', '0958.HK', '0906.HK']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "['1136.HK', '1768.HK', '1688.HK', '0049.HK', '0349.HK', '0203.HK', '0773.HK', '2626.HK', '0930.HK', '2332.HK']: YFPricesMissingError('possibly delisted; no price data found  (1d 2009-01-01 -> 2026-01-19)')\n",
      "['2066.HK']: YFPricesMissingError('possibly delisted; no price data found  (1d 2009-01-01 -> 2026-01-20) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Download complete. Shape: (4354, 4735)\n",
      "[-] Extracting and aligning components...\n",
      "[✓] MarketData object created..\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T22:27:16.900507Z",
     "start_time": "2026-01-19T22:27:15.849163Z"
    }
   },
   "cell_type": "code",
   "source": "# save_market_data_to_parquet(hsci_all_md, get_processed_dir() / 'market/hsci_ohlc.parquet')",
   "id": "dc156f112a4b2dc0",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:12:20.560362Z",
     "start_time": "2026-01-20T14:12:20.554386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tickers = list(hsci_all_md.close.columns)\n",
    "# \n",
    "# raw = download_mcap_snapshot_yf(tickers)\n",
    "# snap = finalize_mcap_snapshot(raw, asof_date=hsci_all_md.close.index[-1])\n",
    "# \n",
    "# shares = compute_estimated_shares(hsci_all_md.close, snap, px_asof=hsci_all_md.close.index[-1])\n",
    "# mcap = build_historical_mcap(hsci_all_md.close, shares)\n",
    "\n",
    "\n",
    "# save_mkt_cp_snapshot(snap, \"data/processed/reference/hsci_mktcap_snapshot.parquet\")\n",
    "\n",
    "# save_estimated_shares(shares, \"data/processed/reference/hsci_estimated_shares.parquet\",\n",
    "#                       asof_date=snap[\"asof_date\"].iloc[0], px_asof_date=hsci_all_md.close.index[-1])\n",
    "\n",
    "\n",
    "# save_historical_mcap_parquet(mcap, \"data/processed/reference/hsci_mcap_approx.parquet\")\n"
   ],
   "id": "df4fdeb73b198696",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:02:57.385654Z",
     "start_time": "2026-01-19T23:02:57.140232Z"
    }
   },
   "cell_type": "code",
   "source": "# save_historical_mcap_parquet(historical_mcap, get_processed_dir() / 'fundamentals/hsci_mkt_cp.parquet')",
   "id": "41b7c38cb7cc9608",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/henrywzh/Desktop/Quant/Research/data/processed/fundamentals/hsci_mkt_cap.parquet')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:07:57.310123Z",
     "start_time": "2026-01-19T23:07:57.278905Z"
    }
   },
   "cell_type": "code",
   "source": "# save_estimated_shares(estimated_shares, get_processed_dir() / 'fundamentals/hsci_estimated_shares.parquet')",
   "id": "29be8bae7200c407",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/henrywzh/Desktop/Quant/Research/data/processed/fundamentals/hsci_estimated_shares.parquet')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "430cbf8bd90caa36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
