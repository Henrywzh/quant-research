{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:40:08.900404Z",
     "start_time": "2026-01-20T14:40:08.897427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Iterable, Literal, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qresearch.data.io import load_hsci_2008_components, load_hsci_components_history, load_market_data_from_parquet\n",
    "from qresearch.data.utils import get_processed_dir, save_market_data_to_csv, load_market_data_from_csv\n",
    "from qresearch.data.yfinance import download_market_data"
   ],
   "id": "fa7784425dd5eab3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-20T14:40:09.336793Z",
     "start_time": "2026-01-20T14:40:09.315789Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# 0) Core utilities: naming + label/feature building\n",
    "# ============================================================\n",
    "def ensure_names(close_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    close = close_df.sort_index().copy()\n",
    "    close.index = pd.to_datetime(close.index)\n",
    "    close.index.name = \"date\"\n",
    "    close.columns.name = \"ticker\"\n",
    "    return close\n",
    "\n",
    "\n",
    "def make_fwd_return(close_df: pd.DataFrame, H: int = 5) -> pd.DataFrame:\n",
    "    close = ensure_names(close_df)\n",
    "    return close.shift(-H) / close - 1.0\n",
    "\n",
    "\n",
    "def rsi_wilder(close: pd.DataFrame, period: int = 14) -> pd.DataFrame:\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0.0)\n",
    "    down = (-delta).clip(lower=0.0)\n",
    "    roll_up = up.ewm(alpha=1 / period, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1 / period, adjust=False).mean()\n",
    "    rs = roll_up / roll_down\n",
    "    return 100.0 - (100.0 / (1.0 + rs))\n",
    "\n",
    "\n",
    "def make_features_default(close_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    close = ensure_names(close_df)\n",
    "\n",
    "    ma5 = close.rolling(5).mean()\n",
    "    ma10 = close.rolling(10).mean()\n",
    "    ma20 = close.rolling(20).mean()\n",
    "\n",
    "    close_ma_diff_5 = close / ma5 - 1.0\n",
    "    close_ma_diff_10 = close / ma10 - 1.0\n",
    "    close_ma_diff_20 = close / ma20 - 1.0\n",
    "\n",
    "    max_growth_5 = close / close.shift(5) - 1.0\n",
    "\n",
    "    rsi = rsi_wilder(close, period=14)\n",
    "\n",
    "    roll_max_120 = close.rolling(120).max()\n",
    "    dd = close / roll_max_120 - 1.0\n",
    "    max_drawdown_120 = dd.rolling(120).min()\n",
    "\n",
    "    return {\n",
    "        \"close_ma_diff_5\": close_ma_diff_5,\n",
    "        \"max_growth_5\": max_growth_5,\n",
    "        \"close_ma_diff_10\": close_ma_diff_10,\n",
    "        \"rsi\": rsi,\n",
    "        \"close_ma_diff_20\": close_ma_diff_20,\n",
    "        \"max_drawdown_120\": max_drawdown_120,\n",
    "    }\n",
    "\n",
    "\n",
    "def wide_to_panel(features: Dict[str, pd.DataFrame], y_wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert wide feature dict + wide y into a long panel:\n",
    "    columns: date, ticker, <feature...>, y\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for name, df in features.items():\n",
    "        s = df.stack(future_stack=True).rename(name)\n",
    "        parts.append(s)\n",
    "\n",
    "    X = pd.concat(parts, axis=1)\n",
    "    y = y_wide.stack(future_stack=True).rename(\"y\")\n",
    "\n",
    "    panel = pd.concat([X, y], axis=1)\n",
    "    panel.index = panel.index.set_names([\"date\", \"ticker\"])\n",
    "    panel = panel.reset_index()\n",
    "    panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
    "    return panel\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Cross-sectional preprocessing\n",
    "# ============================================================\n",
    "def cs_winsorize_zscore(panel: pd.DataFrame, feature_cols: list[str], q_lo: float = 0.01, q_hi: float = 0.99) -> pd.DataFrame:\n",
    "    out = panel.copy()\n",
    "    for c in feature_cols:\n",
    "        g = out.groupby(\"date\")[c]\n",
    "        lo = g.transform(lambda s: s.quantile(q_lo))\n",
    "        hi = g.transform(lambda s: s.quantile(q_hi))\n",
    "        x = out[c].clip(lo, hi)\n",
    "        mu = x.groupby(out[\"date\"]).transform(\"mean\")\n",
    "        sd = x.groupby(out[\"date\"]).transform(\"std\")\n",
    "        out[c] = (x - mu) / sd\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def flip_negative_ic(panel: pd.DataFrame, feature_cols: list[str]) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Pooled Spearman IC between each feature and y.\n",
    "    If IC < 0, flip sign to enforce positive direction.\n",
    "    \"\"\"\n",
    "    out = panel.copy()\n",
    "    ic_map: Dict[str, float] = {}\n",
    "    for c in feature_cols:\n",
    "        ic = out[c].corr(out[\"y\"], method=\"spearman\")\n",
    "        ic_map[c] = float(ic) if ic is not None else np.nan\n",
    "        if ic is not None and ic < 0:\n",
    "            out[c] = -out[c]\n",
    "    return out, ic_map\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Walk-forward split (2y train / 1m test + purge H)\n",
    "# ============================================================\n",
    "def walk_forward_splits(\n",
    "    dates: Iterable[pd.Timestamp],\n",
    "    train_years: int = 2,\n",
    "    test_months: int = 1,\n",
    "    H: int = 5,\n",
    "    min_train_days: int = 100,\n",
    "    min_test_days: int = 5,\n",
    "):\n",
    "    dates = pd.DatetimeIndex(sorted(pd.DatetimeIndex(dates).unique()))\n",
    "    start = dates.min()\n",
    "    end = dates.max()\n",
    "\n",
    "    cur_test_start = start + pd.DateOffset(years=train_years)\n",
    "    while cur_test_start < end:\n",
    "        cur_test_end = cur_test_start + pd.DateOffset(months=test_months)\n",
    "\n",
    "        test_dates = dates[(dates >= cur_test_start) & (dates < cur_test_end)]\n",
    "        if len(test_dates) == 0:\n",
    "            cur_test_start = cur_test_end\n",
    "            continue\n",
    "\n",
    "        test_start = test_dates.min()\n",
    "        pos = dates.get_indexer([test_start])[0]\n",
    "        train_end_pos = max(pos - H, 0)  # purge H days\n",
    "        train_end = dates[train_end_pos]\n",
    "\n",
    "        train_start = test_start - pd.DateOffset(years=train_years)\n",
    "        train_dates = dates[(dates >= train_start) & (dates <= train_end)]\n",
    "\n",
    "        if len(train_dates) >= min_train_days and len(test_dates) >= min_test_days:\n",
    "            yield train_dates, test_dates\n",
    "\n",
    "        cur_test_start = cur_test_end\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Model adapters (single interface)\n",
    "# ============================================================\n",
    "@dataclass(frozen=True)\n",
    "class FitPredictContext:\n",
    "    feature_cols: list[str]\n",
    "    top_k: int\n",
    "    H: int\n",
    "\n",
    "\n",
    "FitPredictFn = Callable[\n",
    "    [pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, np.ndarray, FitPredictContext, dict],\n",
    "    np.ndarray\n",
    "]\n",
    "\"\"\"\n",
    "Signature:\n",
    "    predict = fit_predict(\n",
    "        train_df, test_df,\n",
    "        X_train, y_train, group_train,\n",
    "        X_test, group_test,\n",
    "        ctx, params\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "def make_relevance_from_y(\n",
    "    panel: pd.DataFrame,\n",
    "    n_bins: int = 5,\n",
    "    col_y: str = \"y\",\n",
    "    col_date: str = \"date\",\n",
    "    min_per_day: int = 20,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert continuous forward returns y into integer relevance labels per day.\n",
    "\n",
    "    Why:\n",
    "    - LightGBM LambdaRank expects integer labels (relevance grades).\n",
    "    - Relevance must be cross-sectional per query group (here: per date).\n",
    "\n",
    "    Robustness:\n",
    "    - Handles ties / insufficient unique values / small universes.\n",
    "    - Guarantees output is integer in [0, n_bins-1] where defined.\n",
    "    - Returns pd.Series aligned to panel.index (same length).\n",
    "\n",
    "    Convention:\n",
    "    - Higher y => higher relevance (better).\n",
    "    \"\"\"\n",
    "    if n_bins < 2:\n",
    "        raise ValueError(\"n_bins must be >= 2\")\n",
    "\n",
    "    y = panel[col_y]\n",
    "    d = panel[col_date]\n",
    "\n",
    "    def _bin_one_day(s: pd.Series) -> pd.Series:\n",
    "        # s is y for one date, indexed by panel index rows\n",
    "        s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        # Not enough names to create stable bins\n",
    "        if len(s) < max(min_per_day, n_bins):\n",
    "            return pd.Series(index=s.index, data=np.nan)\n",
    "\n",
    "        # If too many ties (e.g., many zeros), qcut can fail or drop bins\n",
    "        # Strategy: rank then map ranks into bins deterministically.\n",
    "        r = s.rank(method=\"average\")  # ascending rank: low y -> small rank\n",
    "        # Convert rank percentiles to bins\n",
    "        # bin_id in 0..n_bins-1\n",
    "        bin_id = np.floor((r - 1) / max(len(r) / n_bins, 1.0))\n",
    "        bin_id = bin_id.clip(0, n_bins - 1)\n",
    "\n",
    "        # Ensure integer dtype but allow NaN at higher level; return Int64 for safety\n",
    "        return pd.Series(index=s.index, data=bin_id.astype(np.int64))\n",
    "\n",
    "    rel = panel.groupby(d, sort=False)[col_y].apply(_bin_one_day)\n",
    "    # groupby/apply returns a multiindex series; drop the group level so it aligns with original rows\n",
    "    rel = rel.reset_index(level=0, drop=True)\n",
    "\n",
    "    # Align to full panel index: rows that were NaN/dropped become NaN here\n",
    "    rel = rel.reindex(panel.index)\n",
    "\n",
    "    # Use pandas nullable Int64 to avoid int-casting NaN error.\n",
    "    return rel.astype(\"Int64\")\n",
    "\n",
    "\n",
    "def fit_predict_xgb_ranker(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    group_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    group_test: np.ndarray,\n",
    "    ctx: FitPredictContext,\n",
    "    params: dict,\n",
    ") -> np.ndarray:\n",
    "    from xgboost import XGBRanker\n",
    "\n",
    "    # map friendly params -> XGBRanker init\n",
    "    model = XGBRanker(**params)\n",
    "    model.fit(X_train, y_train, group=group_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "\n",
    "def fit_predict_lgb_lambdarank(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    group_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    group_test: np.ndarray,\n",
    "    ctx: FitPredictContext,\n",
    "    params: dict,\n",
    ") -> np.ndarray:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    # LightGBM needs Dataset + group\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train, group=group_train, free_raw_data=True)\n",
    "    dvalid = lgb.Dataset(X_test,  label=np.zeros_like(X_test[:, 0]), group=group_test, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "    # training params\n",
    "    lgb_params = params.get(\"lgb_params\", {})\n",
    "    num_boost_round = params.get(\"num_boost_round\", 2000)\n",
    "    early_stopping_rounds = params.get(\"early_stopping_rounds\", 50)\n",
    "\n",
    "    # ensure objective/metric exist\n",
    "    lgb_params = {\n",
    "        \"objective\": \"lambdarank\",\n",
    "        \"metric\": \"ndcg\",\n",
    "        \"verbosity\": -1,\n",
    "        \"ndcg_eval_at\": [ctx.top_k],\n",
    "        **lgb_params,\n",
    "    }\n",
    "\n",
    "    booster = lgb.train(\n",
    "        lgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[dvalid],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False)],\n",
    "    )\n",
    "    return booster.predict(X_test, num_iteration=booster.best_iteration)\n",
    "\n",
    "\n",
    "MODEL_REGISTRY: Dict[str, FitPredictFn] = {\n",
    "    \"xgb_ranker\": fit_predict_xgb_ranker,\n",
    "    \"lgb_lambdarank\": fit_predict_lgb_lambdarank,\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Shared evaluation\n",
    "# ============================================================\n",
    "def eval_rankic(oos: pd.DataFrame) -> Dict[str, object]:\n",
    "    rankic_by_day = oos.groupby(\"date\", group_keys=False)[[\"score\", \"y\"]].apply(lambda g: g[\"score\"].corr(g[\"y\"], method=\"spearman\"))\n",
    "    mean = float(rankic_by_day.mean())\n",
    "    std = float(rankic_by_day.std(ddof=0))\n",
    "    ir = float(mean / std) if std > 0 else np.nan\n",
    "    return {\"rankic_by_day\": rankic_by_day, \"rankic_mean\": mean, \"rankic_ir\": ir}\n",
    "\n",
    "\n",
    "def eval_topk_proxy(oos: pd.DataFrame, top_k: int) -> Dict[str, object]:\n",
    "    # Diagnostic only: mean of forward H-day return y among top-k scores\n",
    "    topk_fwd = oos.groupby(\"date\", group_keys=False)[[\"score\", \"y\"]].apply(lambda g: g.nlargest(top_k, \"score\")[\"y\"].mean())\n",
    "    topk_eq = (1 + topk_fwd.fillna(0)).cumprod()\n",
    "    return {\"topk_fwd\": topk_fwd, \"topk_fwd_eq\": topk_eq}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) The unified workflow (minimal duplication)\n",
    "# ============================================================\n",
    "def run_rank_workflow(\n",
    "    close_df: pd.DataFrame,\n",
    "    model_name: Literal[\"xgb_ranker\", \"lgb_lambdarank\"],\n",
    "    model_params: dict,\n",
    "    H: int = 5,\n",
    "    top_k: int = 7,\n",
    "    train_years: int = 2,\n",
    "    test_months: int = 1,\n",
    "    preprocess: bool = True,\n",
    "    align_feature_sign: bool = True,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Unified workflow for cross-sectional ranking models (walk-forward).\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    close_df:\n",
    "        Wide close price table: index=Date, columns=Ticker.\n",
    "        Features and y are computed using only information up to close[t].\n",
    "    model_name:\n",
    "        Which ranker to use (delegated to MODEL_REGISTRY).\n",
    "    model_params:\n",
    "        Model-specific parameters, plus optional workflow knobs like:\n",
    "          - relevance_bins (for lgb_lambdarank)\n",
    "    H:\n",
    "        Forward horizon: y[t] = fwd return over the holding window aligned to entry rule.\n",
    "    top_k:\n",
    "        Used by eval_topk_proxy (diagnostic), and can be used by the ranker objective.\n",
    "    train_years / test_months:\n",
    "        Walk-forward split definition.\n",
    "    preprocess:\n",
    "        Cross-sectional winsorize + zscore per date (recommended).\n",
    "    align_feature_sign:\n",
    "        Flip features with negative pooled IC so “higher is better” consistently.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "        - pooled_ic (optional)\n",
    "        - oos (long panel of predictions)\n",
    "        - oos_wide (date x ticker score table; directly usable as a signal)\n",
    "        - rankic stats\n",
    "        - topk proxy stats\n",
    "        - metadata\n",
    "    \"\"\"\n",
    "    if model_name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"Unknown model_name={model_name}. Available: {list(MODEL_REGISTRY.keys())}\")\n",
    "\n",
    "    fit_predict = MODEL_REGISTRY[model_name]\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Build a single shared panel: (date, ticker) rows with features + y\n",
    "    # -------------------------\n",
    "    feats = make_features_default(close_df)\n",
    "    y_wide = make_fwd_return(close_df, H=H)\n",
    "\n",
    "    panel = wide_to_panel(feats, y_wide)  # expects columns: date, ticker, <features...>, y\n",
    "\n",
    "    feature_cols = list(feats.keys())\n",
    "\n",
    "    # Hard clean y to avoid inf contaminating downstream ops\n",
    "    # (You can optionally extend to features, but you already dropna later.)\n",
    "    panel = panel[np.isfinite(panel[\"y\"].to_numpy())].copy()\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) LightGBM LambdaRank needs integer relevance grades per query group (date)\n",
    "    # -------------------------\n",
    "    if model_name == \"lgb_lambdarank\":\n",
    "        n_bins = int(model_params.get(\"relevance_bins\", 5))\n",
    "        panel = panel.copy()\n",
    "        panel[\"rel\"] = make_relevance_from_y(panel, n_bins=n_bins, col_y=\"y\", col_date=\"date\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Drop NA rows from rolling windows + end-of-sample horizon\n",
    "    # -------------------------\n",
    "    drop_cols = [\"y\"] + feature_cols\n",
    "    if model_name == \"lgb_lambdarank\":\n",
    "        # rel is allowed to be NA for bad dates; but training cannot.\n",
    "        drop_cols += [\"rel\"]\n",
    "\n",
    "    panel = panel.dropna(subset=drop_cols)\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Optional cross-sectional preprocessing (per date)\n",
    "    # -------------------------\n",
    "    if preprocess:\n",
    "        panel = cs_winsorize_zscore(panel, feature_cols)\n",
    "\n",
    "    pooled_ic = None\n",
    "    if align_feature_sign:\n",
    "        panel, pooled_ic = flip_negative_ic(panel, feature_cols)\n",
    "\n",
    "    # Ensure chronological order for splits\n",
    "    all_dates = pd.DatetimeIndex(panel[\"date\"].unique()).sort_values()\n",
    "\n",
    "    ctx = FitPredictContext(feature_cols=feature_cols, top_k=top_k, H=H)\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) Walk-forward: train -> predict -> collect OOS scores\n",
    "    # -------------------------\n",
    "    oos_rows = []\n",
    "\n",
    "    for train_dates, test_dates in walk_forward_splits(\n",
    "        all_dates,\n",
    "        train_years=train_years,\n",
    "        test_months=test_months,\n",
    "        H=H,\n",
    "    ):\n",
    "        train = panel[panel[\"date\"].isin(train_dates)].sort_values([\"date\", \"ticker\"])\n",
    "        test  = panel[panel[\"date\"].isin(test_dates)].sort_values([\"date\", \"ticker\"])\n",
    "\n",
    "        # Guard: skip empty splits (can happen if dates get filtered by NA drops)\n",
    "        if len(train) == 0 or len(test) == 0:\n",
    "            continue\n",
    "\n",
    "        # Group sizes: one query group per date\n",
    "        group_train = train.groupby(\"date\").size().to_numpy()\n",
    "        group_test  = test.groupby(\"date\").size().to_numpy()\n",
    "\n",
    "        X_train = train[feature_cols].to_numpy()\n",
    "        X_test  = test[feature_cols].to_numpy()\n",
    "\n",
    "        # y is always continuous forward return (kept for evaluation)\n",
    "        y_test = test[\"y\"].to_numpy()\n",
    "\n",
    "        # For training target:\n",
    "        # - XGB ranker can train directly on continuous y (pairwise)\n",
    "        # - LGBM lambdarank typically expects relevance grades (int)\n",
    "        if model_name == \"lgb_lambdarank\":\n",
    "            y_train_model = train[\"rel\"].to_numpy()\n",
    "        else:\n",
    "            y_train_model = train[\"y\"].to_numpy()\n",
    "\n",
    "        score = fit_predict(\n",
    "            train, test,\n",
    "            X_train, y_train_model, group_train,\n",
    "            X_test, group_test,\n",
    "            ctx, model_params,\n",
    "        )\n",
    "\n",
    "        tmp = test[[\"date\", \"ticker\"]].copy()\n",
    "        tmp[\"score\"] = score\n",
    "        tmp[\"y\"] = y_test\n",
    "        oos_rows.append(tmp)\n",
    "\n",
    "    if not oos_rows:\n",
    "        raise RuntimeError(\"No OOS rows produced. Check splits/NA filtering/universe coverage.\")\n",
    "\n",
    "    oos = (\n",
    "        pd.concat(oos_rows, ignore_index=True)\n",
    "        .sort_values([\"date\", \"score\"], ascending=[True, False])\n",
    "    )\n",
    "\n",
    "    # Wide score table: convenient “signal” to plug into bucket backtest / tearsheet\n",
    "    oos_wide = oos.pivot(index=\"date\", columns=\"ticker\", values=\"score\").sort_index()\n",
    "\n",
    "    rank_metrics = eval_rankic(oos)\n",
    "    topk_metrics = eval_topk_proxy(oos, top_k=top_k)\n",
    "\n",
    "    return {\n",
    "        \"pooled_ic\": pooled_ic,\n",
    "        \"oos\": oos,\n",
    "        \"oos_wide\": oos_wide,\n",
    "        **rank_metrics,\n",
    "        **topk_metrics,\n",
    "        \"model_name\": model_name,\n",
    "        \"model_params\": model_params,\n",
    "        \"H\": H,\n",
    "        \"top_k\": top_k,\n",
    "        \"train_years\": train_years,\n",
    "        \"test_months\": test_months,\n",
    "        \"preprocess\": preprocess,\n",
    "        \"align_feature_sign\": align_feature_sign,\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:40:10.624893Z",
     "start_time": "2026-01-20T14:40:10.622210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import yfinance as yf\n",
    "# \n",
    "# processed_dir = get_processed_dir()\n",
    "# \n",
    "# hsci_stocks = pd.read_csv(f'{processed_dir}/hsci_components.csv')\n",
    "# \n",
    "# START_DATE = \"2009-01-01\"\n",
    "# END_DATE   = None\n",
    "# \n",
    "# def code_int_to_hk(code: int) -> str:\n",
    "#     return f\"{int(code):04d}.HK\"\n",
    "# \n",
    "# UNIVERSE = hsci_stocks[\"Stock Code\"].apply(code_int_to_hk).tolist()\n",
    "\n",
    "# price_df = download_market_data(UNIVERSE, start=START_DATE, end=END_DATE)"
   ],
   "id": "9366504fff53b78a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:40:10.830348Z",
     "start_time": "2026-01-20T14:40:10.828695Z"
    }
   },
   "cell_type": "code",
   "source": "# save_market_data_to_csv(price_df, f'{processed_dir}/hsci_ohlc.csv')",
   "id": "8e055b7ca7e70f4c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:40:11.639149Z",
     "start_time": "2026-01-20T14:40:11.190178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hsci_md = load_market_data_from_parquet(f'{get_processed_dir()}/market/hsci_ohlc.parquet')\n",
    "hsci_md.close.head()"
   ],
   "id": "cf5b54d486eb0659",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ticker        0001.HK    0002.HK   0003.HK   0004.HK    0005.HK    0006.HK  \\\n",
       "Date                                                                         \n",
       "2009-01-02  24.847145  25.940811  2.589432  2.421850  35.178757  14.836930   \n",
       "2009-01-05  26.171890  25.544769  2.589432  2.498910  35.407196  14.559281   \n",
       "2009-01-06  26.333448  25.445759  2.563708  2.443867  35.155895  14.593985   \n",
       "2009-01-07  25.444902  25.445759  2.559422  2.471387  34.265007  14.264276   \n",
       "2009-01-08  24.734060  25.544769  2.585145  2.311766  33.808163  14.611338   \n",
       "\n",
       "Ticker       0008.HK    0010.HK    0011.HK   0012.HK  ...  9987.HK  9988.HK  \\\n",
       "Date                                                  ...                     \n",
       "2009-01-02  0.693667  12.779734  46.658382  7.983262  ...      NaN      NaN   \n",
       "2009-01-05  0.661623  13.275274  45.945343  8.114780  ...      NaN      NaN   \n",
       "2009-01-06  0.669163  13.979462  45.677982  8.469888  ...      NaN      NaN   \n",
       "2009-01-07  0.659738  14.657570  44.563885  8.654012  ...      NaN      NaN   \n",
       "2009-01-08  0.688012  13.744731  45.276897  8.535645  ...      NaN      NaN   \n",
       "\n",
       "Ticker      9990.HK  9991.HK  9992.HK  9993.HK  9995.HK  9996.HK  9997.HK  \\\n",
       "Date                                                                        \n",
       "2009-01-02      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2009-01-05      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2009-01-06      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2009-01-07      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2009-01-08      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "Ticker      9999.HK  \n",
       "Date                 \n",
       "2009-01-02      NaN  \n",
       "2009-01-05      NaN  \n",
       "2009-01-06      NaN  \n",
       "2009-01-07      NaN  \n",
       "2009-01-08      NaN  \n",
       "\n",
       "[5 rows x 934 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>0001.HK</th>\n",
       "      <th>0002.HK</th>\n",
       "      <th>0003.HK</th>\n",
       "      <th>0004.HK</th>\n",
       "      <th>0005.HK</th>\n",
       "      <th>0006.HK</th>\n",
       "      <th>0008.HK</th>\n",
       "      <th>0010.HK</th>\n",
       "      <th>0011.HK</th>\n",
       "      <th>0012.HK</th>\n",
       "      <th>...</th>\n",
       "      <th>9987.HK</th>\n",
       "      <th>9988.HK</th>\n",
       "      <th>9990.HK</th>\n",
       "      <th>9991.HK</th>\n",
       "      <th>9992.HK</th>\n",
       "      <th>9993.HK</th>\n",
       "      <th>9995.HK</th>\n",
       "      <th>9996.HK</th>\n",
       "      <th>9997.HK</th>\n",
       "      <th>9999.HK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>24.847145</td>\n",
       "      <td>25.940811</td>\n",
       "      <td>2.589432</td>\n",
       "      <td>2.421850</td>\n",
       "      <td>35.178757</td>\n",
       "      <td>14.836930</td>\n",
       "      <td>0.693667</td>\n",
       "      <td>12.779734</td>\n",
       "      <td>46.658382</td>\n",
       "      <td>7.983262</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>26.171890</td>\n",
       "      <td>25.544769</td>\n",
       "      <td>2.589432</td>\n",
       "      <td>2.498910</td>\n",
       "      <td>35.407196</td>\n",
       "      <td>14.559281</td>\n",
       "      <td>0.661623</td>\n",
       "      <td>13.275274</td>\n",
       "      <td>45.945343</td>\n",
       "      <td>8.114780</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>26.333448</td>\n",
       "      <td>25.445759</td>\n",
       "      <td>2.563708</td>\n",
       "      <td>2.443867</td>\n",
       "      <td>35.155895</td>\n",
       "      <td>14.593985</td>\n",
       "      <td>0.669163</td>\n",
       "      <td>13.979462</td>\n",
       "      <td>45.677982</td>\n",
       "      <td>8.469888</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>25.444902</td>\n",
       "      <td>25.445759</td>\n",
       "      <td>2.559422</td>\n",
       "      <td>2.471387</td>\n",
       "      <td>34.265007</td>\n",
       "      <td>14.264276</td>\n",
       "      <td>0.659738</td>\n",
       "      <td>14.657570</td>\n",
       "      <td>44.563885</td>\n",
       "      <td>8.654012</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-08</th>\n",
       "      <td>24.734060</td>\n",
       "      <td>25.544769</td>\n",
       "      <td>2.585145</td>\n",
       "      <td>2.311766</td>\n",
       "      <td>33.808163</td>\n",
       "      <td>14.611338</td>\n",
       "      <td>0.688012</td>\n",
       "      <td>13.744731</td>\n",
       "      <td>45.276897</td>\n",
       "      <td>8.535645</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 934 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:54:40.118442Z",
     "start_time": "2026-01-19T21:54:40.116304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hsi_md = download_market_data(\"^HSI\", START_DATE, END_DATE)\n",
    "# \n",
    "# # Extract close matrices\n",
    "# close = hsci_md.close.sort_index()\n",
    "# # close.to_csv(f'{grandparent}/data/processed/hsci_close.csv')\n",
    "# benchmark_close = hsi_md.close.sort_index()\n",
    "# \n",
    "# # Common calendar (optional but recommended)\n",
    "# common_dates = close.index.intersection(benchmark_close.index)\n",
    "# close_df = close.loc[common_dates]\n",
    "# benchmark_close = benchmark_close.loc[common_dates]\n",
    "\n",
    "# from qresearch.backtest.buckets import make_tearsheet\n",
    "# \n",
    "# lgb_params = {\n",
    "#   \"relevance_bins\": 5,\n",
    "#   \"lgb_params\": {\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"num_leaves\": 31,\n",
    "#     \"min_data_in_leaf\": 300,\n",
    "#     \"feature_fraction\": 0.8,\n",
    "#     \"bagging_fraction\": 0.8,\n",
    "#     \"bagging_freq\": 1,\n",
    "#     \"lambda_l2\": 2.0,\n",
    "#     \"min_gain_to_split\": 0.0,\n",
    "#   },\n",
    "#   \"num_boost_round\": 800,\n",
    "# }\n",
    "# \n",
    "# out = run_rank_workflow(\n",
    "#     close_df=close_df,\n",
    "#     model_name=\"lgb_lambdarank\",\n",
    "#     model_params=lgb_params,\n",
    "#     H=5,\n",
    "#     top_k=10,\n",
    "# )\n",
    "# \n",
    "# signal_ml = out[\"oos_wide\"]  # date x ticker model scores\n",
    "# \n",
    "# rep = make_tearsheet(\n",
    "#     price_df=price_df,\n",
    "#     signal=signal_ml,\n",
    "#     H=5,\n",
    "#     n_buckets=20,\n",
    "#     entry_mode=\"next_close\",\n",
    "#     benchmark_price=hsi,  # optional\n",
    "#     benchmark_name=\"^HSI\",\n",
    "# )"
   ],
   "id": "7342421d4a8ef2cd",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Market Cap, Market Data",
   "id": "10e56d05e6f27c82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:55:00.238465Z",
     "start_time": "2026-01-19T21:55:00.211097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from qresearch.universe.hsci import build_hsci_ticker_master\n",
    "# \n",
    "# all_df = load_hsci_components_history()\n",
    "# hsci_2008_df = load_hsci_2008_components()\n",
    "# \n",
    "# hsci_ticker_master = build_hsci_ticker_master(all_df, hsci_2008_df)\n",
    "# \n",
    "# tickers = hsci_ticker_master['ticker']\n",
    "# \n",
    "# hsci_ticker_master"
   ],
   "id": "e1fc0c4833ae80a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      ticker  in_seed_2008  in_history_events\n",
       "0    0001.HK          True               True\n",
       "1    0002.HK          True              False\n",
       "2    0003.HK          True              False\n",
       "3    0004.HK          True               True\n",
       "4    0005.HK          True              False\n",
       "..       ...           ...                ...\n",
       "929  9993.HK         False               True\n",
       "930  9995.HK         False               True\n",
       "931  9996.HK         False               True\n",
       "932  9997.HK         False               True\n",
       "933  9999.HK         False               True\n",
       "\n",
       "[934 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>in_seed_2008</th>\n",
       "      <th>in_history_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005.HK</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>9993.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>9995.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>9996.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>9997.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>9999.HK</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>934 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:40:41.105077Z",
     "start_time": "2026-01-20T14:40:41.103432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # ---- market data ----\n",
    "# hsci_all_md = download_market_data(tickers, start=START_DATE, end=END_DATE)"
   ],
   "id": "dcfea2a08a8564b3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T22:27:16.900507Z",
     "start_time": "2026-01-19T22:27:15.849163Z"
    }
   },
   "cell_type": "code",
   "source": "# save_market_data_to_parquet(hsci_all_md, get_processed_dir() / 'market/hsci_ohlc.parquet')",
   "id": "dc156f112a4b2dc0",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:12:20.560362Z",
     "start_time": "2026-01-20T14:12:20.554386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tickers = list(hsci_all_md.close.columns)\n",
    "# \n",
    "# raw = download_mcap_snapshot_yf(tickers)\n",
    "# snap = finalize_mcap_snapshot(raw, asof_date=hsci_all_md.close.index[-1])\n",
    "# \n",
    "# shares = compute_estimated_shares(hsci_all_md.close, snap, px_asof=hsci_all_md.close.index[-1])\n",
    "# mcap = build_historical_mcap(hsci_all_md.close, shares)\n",
    "\n",
    "\n",
    "# save_mkt_cp_snapshot(snap, \"data/processed/reference/hsci_mktcap_snapshot.parquet\")\n",
    "\n",
    "# save_estimated_shares(shares, \"data/processed/reference/hsci_estimated_shares.parquet\",\n",
    "#                       asof_date=snap[\"asof_date\"].iloc[0], px_asof_date=hsci_all_md.close.index[-1])\n",
    "\n",
    "\n",
    "# save_historical_mcap_parquet(mcap, \"data/processed/reference/hsci_mcap_approx.parquet\")\n"
   ],
   "id": "df4fdeb73b198696",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:02:57.385654Z",
     "start_time": "2026-01-19T23:02:57.140232Z"
    }
   },
   "cell_type": "code",
   "source": "# save_historical_mcap_parquet(historical_mcap, get_processed_dir() / 'fundamentals/hsci_mkt_cp.parquet')",
   "id": "41b7c38cb7cc9608",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/henrywzh/Desktop/Quant/Research/data/processed/fundamentals/hsci_mkt_cap.parquet')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:07:57.310123Z",
     "start_time": "2026-01-19T23:07:57.278905Z"
    }
   },
   "cell_type": "code",
   "source": "# save_estimated_shares(estimated_shares, get_processed_dir() / 'fundamentals/hsci_estimated_shares.parquet')",
   "id": "29be8bae7200c407",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/henrywzh/Desktop/Quant/Research/data/processed/fundamentals/hsci_estimated_shares.parquet')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "430cbf8bd90caa36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
