{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:46:20.484301Z",
     "start_time": "2026-01-21T21:46:20.206932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, replace, field\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== library imports (your refactored code) =====\n",
    "import qresearch.signals as qsigs\n",
    "from qresearch.data.utils import get_processed_dir\n",
    "from qresearch.data.yfinance import *\n",
    "from qresearch.backtest.portfolio import *\n",
    "from qresearch.backtest.metrics import *\n",
    "from qresearch.portfolio.weights import *\n",
    "\n",
    "# -----------------------------\n",
    "# Parquet -> MarketData\n",
    "# -----------------------------\n",
    "def _read_wide(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(path)\n",
    "    # Ensure DatetimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    df = df[~df.index.isna()].sort_index()\n",
    "    # Ensure string tickers\n",
    "    df.columns = df.columns.astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_marketdata_from_parquet(\n",
    "    data_dir: str,\n",
    "    tickers: list[str],\n",
    "    start: str,\n",
    "    end: Optional[str] = None,\n",
    "    strict_calendar: bool = True,\n",
    ") -> MarketData:\n",
    "    \"\"\"\n",
    "    Load wide parquet panels and convert to MarketData.\n",
    "\n",
    "    strict_calendar=True:\n",
    "      - use close as master calendar\n",
    "      - drop any dates where ANY selected ticker has missing close\n",
    "    \"\"\"\n",
    "    tickers = [str(t).strip() for t in tickers]\n",
    "\n",
    "    # Required\n",
    "    close = _read_wide(f\"{data_dir}/ohlcv_wide_close.parquet\")\n",
    "\n",
    "    # Optional panels (load if present)\n",
    "    def maybe(panel_name: str) -> pd.DataFrame | None:\n",
    "        p = f\"{data_dir}/ohlcv_wide_{panel_name}.parquet\"\n",
    "        try:\n",
    "            return _read_wide(p)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    open_ = maybe(\"open\")\n",
    "    high = maybe(\"high\")\n",
    "    low = maybe(\"low\")\n",
    "    volume = maybe(\"volume\")\n",
    "    turnover = maybe(\"turnover\")\n",
    "    pct_chg = maybe(\"pct_chg\")  # parquet name from Step 1–5 pipeline\n",
    "\n",
    "    # Subset to tickers\n",
    "    missing = [t for t in tickers if t not in close.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"These tickers are not in close parquet columns: {missing}\")\n",
    "\n",
    "    close = close[tickers].copy()\n",
    "\n",
    "    # Date slicing\n",
    "    idx = close.index\n",
    "    start_dt = pd.to_datetime(start)\n",
    "    end_dt = pd.to_datetime(end) if end is not None else idx.max()\n",
    "    close = close.loc[(close.index >= start_dt) & (close.index <= end_dt)]\n",
    "\n",
    "    # Master calendar\n",
    "    cal = close.index\n",
    "\n",
    "    def align(panel: pd.DataFrame | None) -> pd.DataFrame | None:\n",
    "        if panel is None:\n",
    "            return None\n",
    "        # If the panel lacks some tickers, keep intersection only\n",
    "        cols = [t for t in tickers if t in panel.columns]\n",
    "        if not cols:\n",
    "            return None\n",
    "        out = panel[cols].reindex(cal)\n",
    "        # If some tickers missing in panel (e.g., turnover not available), expand to full tickers with NaN cols\n",
    "        if set(cols) != set(tickers):\n",
    "            for t in tickers:\n",
    "                if t not in out.columns:\n",
    "                    out[t] = np.nan\n",
    "            out = out[tickers]\n",
    "        return out\n",
    "\n",
    "    open_ = align(open_)\n",
    "    high = align(high)\n",
    "    low = align(low)\n",
    "    volume = align(volume)\n",
    "    turnover = align(turnover)\n",
    "    pct_chg = align(pct_chg)\n",
    "\n",
    "    # Strict calendar cleaning (close must be complete for all tickers)\n",
    "    if strict_calendar:\n",
    "        ok = close.notna().all(axis=1)\n",
    "        close = close.loc[ok]\n",
    "        if open_ is not None:\n",
    "            open_ = open_.loc[ok]\n",
    "        if high is not None:\n",
    "            high = high.loc[ok]\n",
    "        if low is not None:\n",
    "            low = low.loc[ok]\n",
    "        if volume is not None:\n",
    "            volume = volume.loc[ok]\n",
    "        if turnover is not None:\n",
    "            turnover = turnover.loc[ok]\n",
    "        if pct_chg is not None:\n",
    "            pct_chg = pct_chg.loc[ok]\n",
    "\n",
    "    md = MarketData(\n",
    "        close=close,\n",
    "        open=open_,\n",
    "        high=high,\n",
    "        low=low,\n",
    "        volume=volume,\n",
    "        turnover=turnover,\n",
    "        pct_change=pct_chg,  # map parquet pct_chg -> MarketData.pct_change\n",
    "    )\n",
    "    return md"
   ],
   "id": "ff0ec4edef6d87f1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:46:20.597854Z",
     "start_time": "2026-01-21T21:46:20.594904Z"
    }
   },
   "cell_type": "code",
   "source": "DATA_DIR = get_processed_dir() / \"data_cn_etf_universe\"  # change to your parquet directory",
   "id": "fe6177759bf87e0a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:48:04.507373Z",
     "start_time": "2026-01-21T21:48:04.486581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "@dataclass(frozen=True)\n",
    "class UniverseDiagConfig:\n",
    "    report_start: str = \"2015-01-01\"   # Availability 统计从这里开始\n",
    "    report_end: Optional[str] = None   # None -> close.index.max()\n",
    "\n",
    "    window: int = 240                 # corr/cluster 用最近 window 个交易日\n",
    "    min_obs_window: int = 120         # 在 window 内至少有多少非空收益（太少不稳）\n",
    "\n",
    "    universe_size: int = 10           # 目标推荐池大小（也用作 KMeans 的 K）\n",
    "\n",
    "    corr_prune_thr: float = 0.90      # 二次去冗余阈值（对 reps 再做一次）\n",
    "\n",
    "    # KMeans\n",
    "    random_state: int = 42\n",
    "    n_init: int = 20\n",
    "    zscore_per_asset: bool = True     # 强烈建议 True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Core utilities\n",
    "# =========================\n",
    "def _to_simple_returns(close: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"日简单收益；不 pad 填充（避免未来 pandas 默认 pad 的 warning）。\"\"\"\n",
    "    close = close.sort_index()\n",
    "    ret = close.pct_change(fill_method=None)\n",
    "    ret.iloc[0] = 0.0\n",
    "    ret = ret.replace([np.inf, -np.inf], np.nan)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _availability_table(\n",
    "    close: pd.DataFrame,\n",
    "    start: pd.Timestamp,\n",
    "    end: pd.Timestamp,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    对齐你给的 Availability 表定义：\n",
    "    - 使用全局 [start, end] 的交易日历长度作为分母\n",
    "    - ticker 若中间缺一两天，也算 missing\n",
    "    - ticker 若晚出现，早期整段缺失也会计入 missing（这就是你表里后发 ETF missing_rate 大的原因）\n",
    "    \"\"\"\n",
    "    px = close.loc[start:end].copy()\n",
    "    total_days = len(px.index)\n",
    "\n",
    "    rows = []\n",
    "    for t in px.columns:\n",
    "        s = px[t]\n",
    "        n_obs = int(s.notna().sum())\n",
    "        miss = int(s.isna().sum())\n",
    "        missing_rate = (miss / total_days) if total_days > 0 else np.nan\n",
    "\n",
    "        start_date = s.first_valid_index()\n",
    "        end_date = s.last_valid_index()\n",
    "\n",
    "        rows.append({\n",
    "            \"ticker\": t,\n",
    "            \"start_date\": pd.to_datetime(start_date).date() if start_date is not None else pd.NaT,\n",
    "            \"end_date\": pd.to_datetime(end_date).date() if end_date is not None else pd.NaT,\n",
    "            \"n_obs\": n_obs,\n",
    "            \"missing_rate\": float(missing_rate) if pd.notna(missing_rate) else np.nan,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\n",
    "        [\"missing_rate\", \"start_date\", \"ticker\"],\n",
    "        ascending=[True, True, True],\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _corr_summary(ret_win: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    输出：\n",
    "    - corr summary（mean/max/min）\n",
    "    - top correlated pairs\n",
    "    - corr matrix\n",
    "    \"\"\"\n",
    "    corr = ret_win.corr()  # pairwise complete\n",
    "\n",
    "    # summary（对角线不参与）\n",
    "    corr2 = corr.copy()\n",
    "    np.fill_diagonal(corr2.values, np.nan)\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"ticker\": corr2.columns,\n",
    "        \"mean_corr\": corr2.mean(axis=1, skipna=True).values,\n",
    "        \"max_corr\": corr2.max(axis=1, skipna=True).values,\n",
    "        \"min_corr\": corr2.min(axis=1, skipna=True).values,\n",
    "        \"n_pairs\": corr2.notna().sum(axis=1).values,\n",
    "    }).sort_values(\"mean_corr\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # top pairs\n",
    "    pairs = []\n",
    "    cols = corr2.columns.tolist()\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            c = corr2.iat[i, j]\n",
    "            if pd.notna(c):\n",
    "                pairs.append((cols[i], cols[j], float(c)))\n",
    "\n",
    "    top_pairs = (\n",
    "        pd.DataFrame(pairs, columns=[\"a\", \"b\", \"corr\"])\n",
    "        .sort_values(\"corr\", ascending=False)\n",
    "        .head(50)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return summary, top_pairs, corr\n",
    "\n",
    "\n",
    "def _zscore_cols(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    mu = x.mean(axis=0)\n",
    "    sd = x.std(axis=0, ddof=0).replace(0.0, np.nan)\n",
    "    z = (x - mu) / sd\n",
    "    z = z.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return z\n",
    "\n",
    "\n",
    "def _cluster_kmeans(\n",
    "    ret_win: pd.DataFrame,\n",
    "    k: int,\n",
    "    cfg: UniverseDiagConfig,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    KMeans 聚类：\n",
    "    - 特征：每只ETF最近 window 天的收益序列（长度=window）\n",
    "    - 缺失：先 fillna(0)\n",
    "    - 标准化：按资产 zscore（可选）\n",
    "    输出：cluster label（从 1 开始，与你日志一致）\n",
    "    \"\"\"\n",
    "    X = ret_win.copy().fillna(0.0)\n",
    "\n",
    "    if cfg.zscore_per_asset:\n",
    "        X = _zscore_cols(X)\n",
    "\n",
    "    # KMeans 输入 shape=(n_assets, window)\n",
    "    X_assets = X.T  # index=ticker\n",
    "\n",
    "    km = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=cfg.random_state,\n",
    "        n_init=cfg.n_init,\n",
    "    )\n",
    "    labels0 = km.fit_predict(X_assets.values)  # 0..k-1\n",
    "    labels = pd.Series(labels0 + 1, index=X_assets.index, name=\"cluster\")  # 1..k\n",
    "    return labels\n",
    "\n",
    "\n",
    "def _pick_cluster_representatives(\n",
    "    clusters: pd.Series,\n",
    "    availability: pd.DataFrame,\n",
    "    size: int,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    每个 cluster 选一个代表：start_date 最早（与你参考策略一致）。\n",
    "    如果 cluster 数 != size：\n",
    "      - 若 cluster > size：取“成员数最多”的前 size 个 cluster\n",
    "      - 若 cluster < size：先取每簇代表，再按 start_date 早的补足\n",
    "    \"\"\"\n",
    "    # availability indexed by ticker\n",
    "    av = availability.set_index(\"ticker\")\n",
    "\n",
    "    # cluster sizes\n",
    "    cluster_sizes = clusters.value_counts().sort_values(ascending=False)\n",
    "\n",
    "    # 目标 cluster 集合\n",
    "    cluster_ids = cluster_sizes.index.tolist()\n",
    "    if len(cluster_ids) > size:\n",
    "        cluster_ids = cluster_ids[:size]\n",
    "\n",
    "    reps = []\n",
    "    for cid in cluster_ids:\n",
    "        members = clusters[clusters == cid].index.tolist()\n",
    "        # 按 start_date 最早选\n",
    "        sub = av.loc[members].copy()\n",
    "        # start_date 可能 NaT\n",
    "        sub[\"start_date_ts\"] = pd.to_datetime(sub[\"start_date\"], errors=\"coerce\")\n",
    "        sub = sub.sort_values([\"start_date_ts\", \"ticker\"], ascending=[True, True])\n",
    "        reps.append(sub.index[0])\n",
    "\n",
    "    # 如果 reps 不够 size，补：在全体里按 start_date 早补齐（避免重复）\n",
    "    if len(reps) < size:\n",
    "        remaining = [t for t in clusters.index if t not in reps]\n",
    "        sub = av.loc[remaining].copy()\n",
    "        sub[\"start_date_ts\"] = pd.to_datetime(sub[\"start_date\"], errors=\"coerce\")\n",
    "        sub = sub.sort_values([\"start_date_ts\", \"ticker\"], ascending=[True, True])\n",
    "        need = size - len(reps)\n",
    "        reps.extend(sub.index[:need].tolist())\n",
    "\n",
    "    return reps\n",
    "\n",
    "\n",
    "def _corr_prune_reps(\n",
    "    reps: List[str],\n",
    "    corr: pd.DataFrame,\n",
    "    availability: pd.DataFrame,\n",
    "    thr: float,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    在 reps 上再做一次去冗余：\n",
    "    若 corr(a,b) > thr，保留 start_date 更早者。\n",
    "    \"\"\"\n",
    "    av = availability.set_index(\"ticker\")\n",
    "    sd = pd.to_datetime(av[\"start_date\"], errors=\"coerce\")\n",
    "\n",
    "    keep = set(reps)\n",
    "    reps_list = list(reps)\n",
    "\n",
    "    pairs = []\n",
    "    for i in range(len(reps_list)):\n",
    "        for j in range(i + 1, len(reps_list)):\n",
    "            a, b = reps_list[i], reps_list[j]\n",
    "            c = corr.loc[a, b]\n",
    "            if pd.notna(c) and c > thr:\n",
    "                pairs.append((a, b, float(c)))\n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    for a, b, c in pairs:\n",
    "        if a not in keep or b not in keep:\n",
    "            continue\n",
    "        sda = sd.get(a, pd.NaT)\n",
    "        sdb = sd.get(b, pd.NaT)\n",
    "\n",
    "        # 保留更早 start_date；缺失则保留有日期者\n",
    "        if pd.isna(sda) and pd.isna(sdb):\n",
    "            drop = b\n",
    "        elif pd.isna(sda):\n",
    "            drop = a\n",
    "        elif pd.isna(sdb):\n",
    "            drop = b\n",
    "        else:\n",
    "            drop = b if sda <= sdb else a\n",
    "\n",
    "        keep.remove(drop)\n",
    "\n",
    "    # 保持原先顺序输出\n",
    "    out = [t for t in reps if t in keep]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _rolling_26w_corr_for_pair(\n",
    "    close: pd.DataFrame,\n",
    "    a: str,\n",
    "    b: str,\n",
    "    freq: str = \"W-FRI\",\n",
    "    window: int = 26,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    对齐你输出的“Rolling 26W corr”：\n",
    "    - 用周频收益（周复合）计算\n",
    "    - rolling(window) 相关\n",
    "    \"\"\"\n",
    "    px = close[[a, b]].copy().sort_index()\n",
    "    ret_d = px.pct_change(fill_method=None).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # 周复合收益\n",
    "    def _compound(x: pd.Series) -> float:\n",
    "        return float((1.0 + x).prod() - 1.0)\n",
    "\n",
    "    ret_w = ret_d.resample(freq).apply(_compound)\n",
    "    s = ret_w[a].rolling(window).corr(ret_w[b])\n",
    "    return s.dropna()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main: universe diagnosis\n",
    "# =========================\n",
    "def diagnose_universe(close: pd.DataFrame, cfg: UniverseDiagConfig) -> Dict[str, object]:\n",
    "    close = close.sort_index()\n",
    "\n",
    "    report_start = pd.to_datetime(cfg.report_start)\n",
    "    report_end = pd.to_datetime(cfg.report_end) if cfg.report_end else close.index.max()\n",
    "\n",
    "    # 1) Availability\n",
    "    availability = _availability_table(close, report_start, report_end)\n",
    "\n",
    "    # 2) returns + window slice\n",
    "    ret = _to_simple_returns(close)\n",
    "    ret_win = ret.iloc[-cfg.window:].copy()\n",
    "\n",
    "    # 3) min obs in window filter (避免样本太少导致 corr/cluster 不稳)\n",
    "    obs = ret_win.notna().sum(axis=0)\n",
    "    keep_cols = obs[obs >= cfg.min_obs_window].index.tolist()\n",
    "    ret_win = ret_win[keep_cols]\n",
    "\n",
    "    # 4) corr summary + top pairs\n",
    "    corr_summary, top_pairs, corr = _corr_summary(ret_win)\n",
    "\n",
    "    # 5) clustering (K = universe_size)\n",
    "    k = min(cfg.universe_size, ret_win.shape[1])  # 资产不足时保护\n",
    "    clusters = _cluster_kmeans(ret_win, k=k, cfg=cfg)\n",
    "\n",
    "    # 6) reps by earliest start_date in each cluster\n",
    "    availability_kept = availability[availability[\"ticker\"].isin(ret_win.columns)].copy()\n",
    "    reps_raw = _pick_cluster_representatives(clusters, availability_kept, size=k)\n",
    "\n",
    "    # 7) optional corr prune on reps\n",
    "    reps_pruned = _corr_prune_reps(\n",
    "        reps=reps_raw,\n",
    "        corr=corr,\n",
    "        availability=availability_kept,\n",
    "        thr=cfg.corr_prune_thr,\n",
    "    )\n",
    "\n",
    "    # 8) rolling 26W corr for top pair (from top_pairs)\n",
    "    rolling_corr = None\n",
    "    top_pair = None\n",
    "    if len(top_pairs) > 0:\n",
    "        a, b = top_pairs.loc[0, \"a\"], top_pairs.loc[0, \"b\"]\n",
    "        top_pair = (a, b)\n",
    "        rolling_corr = _rolling_26w_corr_for_pair(close, a, b, freq=\"W-FRI\", window=26)\n",
    "\n",
    "    return {\n",
    "        \"availability\": availability,\n",
    "        \"corr_summary\": corr_summary,\n",
    "        \"top_pairs\": top_pairs,\n",
    "        \"clusters\": clusters.sort_values(),\n",
    "        \"suggested_universe_raw\": reps_raw,\n",
    "        \"suggested_universe\": reps_pruned,\n",
    "        \"top_pair\": top_pair,\n",
    "        \"rolling_26w_corr\": rolling_corr,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pretty print (exactly align your log blocks)\n",
    "# =========================\n",
    "def print_universe_report(out: Dict[str, object], universe_size: int = 10) -> None:\n",
    "    availability: pd.DataFrame = out[\"availability\"]\n",
    "    corr_summary: pd.DataFrame = out[\"corr_summary\"]\n",
    "    top_pairs: pd.DataFrame = out[\"top_pairs\"]\n",
    "    clusters: pd.Series = out[\"clusters\"]\n",
    "    suggested: List[str] = out[\"suggested_universe\"]\n",
    "\n",
    "    print(f\"\\n=== Availability (top 15) ===\")\n",
    "    print(availability.head(15).to_string(index=True))\n",
    "\n",
    "    print(f\"\\n=== Corr summary (most redundant first) ===\")\n",
    "    print(corr_summary.head(15).to_string(index=True))\n",
    "\n",
    "    print(f\"\\n=== Top correlated pairs ===\")\n",
    "    print(top_pairs.head(20).to_string(index=True))\n",
    "\n",
    "    print(f\"\\n=== Cluster assignment ===\")\n",
    "    print(clusters.to_string())\n",
    "\n",
    "    # 输出对齐你现在：['...', '...']\n",
    "    sug = suggested[:universe_size]\n",
    "    print(f\"\\n=== Suggested universe (size={len(sug)}) ===\")\n",
    "    print(sug)\n",
    "\n",
    "    # rolling corr\n",
    "    if out.get(\"top_pair\") and out.get(\"rolling_26w_corr\") is not None:\n",
    "        a, b = out[\"top_pair\"]\n",
    "        print(f\"\\nRolling 26W corr for top pair {a}-{b}:\")\n",
    "        print(out[\"rolling_26w_corr\"].tail(10).to_string())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example usage\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 你需要提供 close:\n",
    "    # close: pd.DataFrame\n",
    "    # - index: trading dates\n",
    "    # - columns: tickers like '510880', '159928', ...\n",
    "    #\n",
    "    # 例如：\n",
    "    # close = md.close  # 你的 MarketData close\n",
    "    #\n",
    "    # 这里仅展示调用方式（不会实际运行）\n",
    "    cfg = UniverseDiagConfig(\n",
    "        report_start=\"2015-01-01\",\n",
    "        report_end=None,\n",
    "        window=240,\n",
    "        min_obs_window=120,\n",
    "        universe_size=10,\n",
    "        corr_prune_thr=0.90,\n",
    "        random_state=42,\n",
    "        n_init=20,\n",
    "        zscore_per_asset=True,\n",
    "    )\n",
    "\n",
    "    # out = diagnose_universe(close, cfg)\n",
    "    # print_universe_report(out, universe_size=cfg.universe_size)\n",
    "    pass\n"
   ],
   "id": "8d5d8df08d619b4a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:51:11.207098Z",
     "start_time": "2026-01-21T21:51:11.179411Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "42dfddead32023ca",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:51:39.520291Z",
     "start_time": "2026-01-21T21:51:12.156165Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "792a7c586bab73e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PortfolioBacktestResult(gross_ret=date\n",
      "2019-01-02    0.000000\n",
      "2019-01-03    0.000000\n",
      "2019-01-04    0.000000\n",
      "2019-01-07    0.000000\n",
      "2019-01-08    0.000000\n",
      "                ...   \n",
      "2026-01-15   -0.021169\n",
      "2026-01-16    0.004831\n",
      "2026-01-19   -0.003523\n",
      "2026-01-20   -0.008081\n",
      "2026-01-21    0.047352\n",
      "Length: 1712, dtype: float64, net_ret=date\n",
      "2019-01-02    0.000000\n",
      "2019-01-03    0.000000\n",
      "2019-01-04    0.000000\n",
      "2019-01-07    0.000000\n",
      "2019-01-08    0.000000\n",
      "                ...   \n",
      "2026-01-15   -0.021569\n",
      "2026-01-16    0.004431\n",
      "2026-01-19   -0.003923\n",
      "2026-01-20   -0.008081\n",
      "2026-01-21    0.047352\n",
      "Length: 1712, dtype: float64, equity_gross=date\n",
      "2019-01-02    1.000000\n",
      "2019-01-03    1.000000\n",
      "2019-01-04    1.000000\n",
      "2019-01-07    1.000000\n",
      "2019-01-08    1.000000\n",
      "                ...   \n",
      "2026-01-15    1.075664\n",
      "2026-01-16    1.080861\n",
      "2026-01-19    1.077053\n",
      "2026-01-20    1.068349\n",
      "2026-01-21    1.118938\n",
      "Length: 1712, dtype: float64, equity_net=date\n",
      "2019-01-02    1.000000\n",
      "2019-01-03    1.000000\n",
      "2019-01-04    1.000000\n",
      "2019-01-07    1.000000\n",
      "2019-01-08    1.000000\n",
      "                ...   \n",
      "2026-01-15    0.974819\n",
      "2026-01-16    0.979138\n",
      "2026-01-19    0.975297\n",
      "2026-01-20    0.967416\n",
      "2026-01-21    1.013225\n",
      "Length: 1712, dtype: float64, turnover=date\n",
      "2019-01-02    0.0\n",
      "2019-01-03    0.0\n",
      "2019-01-04    0.0\n",
      "2019-01-07    0.0\n",
      "2019-01-08    0.0\n",
      "             ... \n",
      "2026-01-15    2.0\n",
      "2026-01-16    2.0\n",
      "2026-01-19    2.0\n",
      "2026-01-20    0.0\n",
      "2026-01-21    0.0\n",
      "Length: 1712, dtype: float64, fee=date\n",
      "2019-01-02    0.0000\n",
      "2019-01-03    0.0000\n",
      "2019-01-04    0.0000\n",
      "2019-01-07    0.0000\n",
      "2019-01-08    0.0000\n",
      "               ...  \n",
      "2026-01-15    0.0004\n",
      "2026-01-16    0.0004\n",
      "2026-01-19    0.0004\n",
      "2026-01-20    0.0000\n",
      "2026-01-21    0.0000\n",
      "Length: 1712, dtype: float64, exposure=date\n",
      "2019-01-02    0.0\n",
      "2019-01-03    1.0\n",
      "2019-01-04    1.0\n",
      "2019-01-07    1.0\n",
      "2019-01-08    1.0\n",
      "             ... \n",
      "2026-01-15    1.0\n",
      "2026-01-16    1.0\n",
      "2026-01-19    1.0\n",
      "2026-01-20    1.0\n",
      "2026-01-21    1.0\n",
      "Length: 1712, dtype: float64, cash_weight=date\n",
      "2019-01-02    1.0\n",
      "2019-01-03    0.0\n",
      "2019-01-04    0.0\n",
      "2019-01-07    0.0\n",
      "2019-01-08    0.0\n",
      "             ... \n",
      "2026-01-15    0.0\n",
      "2026-01-16    0.0\n",
      "2026-01-19    0.0\n",
      "2026-01-20    0.0\n",
      "2026-01-21    0.0\n",
      "Length: 1712, dtype: float64, weights_used=ticker      516910  512980  159825  512010  512660  159870  159996  159745  \\\n",
      "date                                                                         \n",
      "2019-01-02     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2019-01-03     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2019-01-04     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2019-01-07     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2019-01-08     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "...            ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "2026-01-15     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2026-01-16     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2026-01-19     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2026-01-20     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2026-01-21     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "ticker      516970  512200  ...  502023  512800  512070  512690  513180  \\\n",
      "date                        ...                                           \n",
      "2019-01-02     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "2019-01-03     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "2019-01-04     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "2019-01-07     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "2019-01-08     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "...            ...     ...  ...     ...     ...     ...     ...     ...   \n",
      "2026-01-15     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "2026-01-16     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "2026-01-19     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "2026-01-20     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "2026-01-21     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "ticker      510410  515030  159928  510230  510880  \n",
      "date                                                \n",
      "2019-01-02     0.0     0.0     0.0     0.0     0.0  \n",
      "2019-01-03     0.0     0.0     0.0     0.0     0.0  \n",
      "2019-01-04     0.0     0.0     0.0     0.0     0.0  \n",
      "2019-01-07     0.0     0.0     0.0     0.0     0.0  \n",
      "2019-01-08     0.0     0.0     0.0     0.0     0.0  \n",
      "...            ...     ...     ...     ...     ...  \n",
      "2026-01-15     0.0     0.0     0.0     0.0     0.0  \n",
      "2026-01-16     0.0     0.0     0.0     0.0     0.0  \n",
      "2026-01-19     0.0     0.0     0.0     0.0     0.0  \n",
      "2026-01-20     0.0     0.0     0.0     0.0     0.0  \n",
      "2026-01-21     0.0     0.0     0.0     0.0     0.0  \n",
      "\n",
      "[1712 rows x 30 columns])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "887540eb6f24b92f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
